{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoumanAhmad448/Improving_Detection_of_Person_using_dense_pooling/blob/master/dense_pooling_with_resnet101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VIlpsN2MErIj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6f02a0d-5043-430a-bbe5-4372907a4141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fiftyone\n",
            "  Downloading fiftyone-0.20.1-py3-none-any.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles (from fiftyone)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting argcomplete (from fiftyone)\n",
            "  Downloading argcomplete-3.0.8-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3 (from fiftyone)\n",
            "  Downloading boto3-1.26.139-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.3.0)\n",
            "Collecting dacite<1.8.0,>=1.6.0 (from fiftyone)\n",
            "  Downloading dacite-1.7.0-py3-none-any.whl (12 kB)\n",
            "Collecting Deprecated (from fiftyone)\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting eventlet (from fiftyone)\n",
            "  Downloading eventlet-0.33.3-py2.py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy (from fiftyone)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.18.3)\n",
            "Collecting hypercorn>=0.13.2 (from fiftyone)\n",
            "  Downloading Hypercorn-0.14.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=3 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.1.2)\n",
            "Collecting kaleido (from fiftyone)\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.7.1)\n",
            "Collecting mongoengine==0.24.2 (from fiftyone)\n",
            "  Downloading mongoengine-0.24.2-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting motor>=2.5 (from fiftyone)\n",
            "  Downloading motor-3.1.2-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fiftyone) (23.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.5.3)\n",
            "Requirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (8.4.0)\n",
            "Requirement already satisfied: plotly>=4.14 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.13.1)\n",
            "Collecting pprintpp (from fiftyone)\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.9.5)\n",
            "Collecting pymongo>=3.12 (from fiftyone)\n",
            "  Downloading pymongo-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2022.7.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from fiftyone) (6.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2022.10.31)\n",
            "Collecting retrying (from fiftyone)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.19.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (67.7.2)\n",
            "Collecting sseclient-py<2,>=1.7.2 (from fiftyone)\n",
            "  Downloading sseclient_py-1.7.2-py2.py3-none-any.whl (8.4 kB)\n",
            "Collecting sse-starlette<1,>=0.10.3 (from fiftyone)\n",
            "  Downloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\n",
            "Collecting starlette==0.20.4 (from fiftyone)\n",
            "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting strawberry-graphql==0.138.1 (from fiftyone)\n",
            "  Downloading strawberry_graphql-0.138.1-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.8.10)\n",
            "Collecting xmltodict (from fiftyone)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting universal-analytics-python3<2,>=1.0.1 (from fiftyone)\n",
            "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n",
            "Collecting fiftyone-brain<0.12,>=0.11 (from fiftyone)\n",
            "  Downloading fiftyone_brain-0.11.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fiftyone-db<0.5,>=0.4 (from fiftyone)\n",
            "  Downloading fiftyone_db-0.4.0-py3-none-manylinux1_x86_64.whl (37.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting voxel51-eta<0.10,>=0.9 (from fiftyone)\n",
            "  Downloading voxel51_eta-0.9.0-py2.py3-none-any.whl (568 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.7.0.72)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette==0.20.4->fiftyone) (3.6.2)\n",
            "Collecting graphql-core<3.3.0,>=3.2.0 (from strawberry-graphql==0.138.1->fiftyone)\n",
            "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (2.8.2)\n",
            "Requirement already satisfied: typing_extensions<5.0.0,>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (4.5.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from fiftyone-brain<0.12,>=0.11->fiftyone) (1.10.1)\n",
            "Collecting h11 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2>=3.1.0 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting priority (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (0.10.2)\n",
            "Collecting wsproto>=0.14.0 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3->fiftyone) (2.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.14->fiftyone) (8.2.2)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo>=3.12->fiftyone)\n",
            "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.10.0 (from universal-analytics-python3<2,>=1.0.1->fiftyone)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from voxel51-eta<0.10,>=0.9->fiftyone)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: glob2 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.10,>=0.9->fiftyone) (0.7)\n",
            "Collecting jsonlines (from voxel51-eta<0.10,>=0.9->fiftyone)\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Collecting py7zr (from voxel51-eta<0.10,>=0.9->fiftyone)\n",
            "  Downloading py7zr-0.20.5-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rarfile (from voxel51-eta<0.10,>=0.9->fiftyone)\n",
            "  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.10,>=0.9->fiftyone) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.10,>=0.9->fiftyone) (1.16.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.10,>=0.9->fiftyone) (2.4.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.10,>=0.9->fiftyone) (4.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.10,>=0.9->fiftyone) (1.26.15)\n",
            "Collecting botocore<1.30.0,>=1.29.139 (from boto3->fiftyone)\n",
            "  Downloading botocore-1.29.139-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->fiftyone)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->fiftyone)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->fiftyone) (1.14.1)\n",
            "Requirement already satisfied: greenlet>=0.3 in /usr/local/lib/python3.10/dist-packages (from eventlet->fiftyone) (2.0.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->fiftyone) (0.2.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (3.0.9)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (1.4.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (3.1.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fiftyone) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fiftyone) (1.3.0)\n",
            "Collecting hyperframe<7,>=6.0 (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2022.12.7)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone)\n",
            "  Downloading httpcore-0.17.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->voxel51-eta<0.10,>=0.9->fiftyone) (23.1.0)\n",
            "Collecting texttable (from py7zr->voxel51-eta<0.10,>=0.9->fiftyone)\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.6.6 (from py7zr->voxel51-eta<0.10,>=0.9->fiftyone)\n",
            "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.14.4 (from py7zr->voxel51-eta<0.10,>=0.9->fiftyone)\n",
            "  Downloading pyzstd-0.15.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (399 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.3/399.3 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr->voxel51-eta<0.10,>=0.9->fiftyone)\n",
            "  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr->voxel51-eta<0.10,>=0.9->fiftyone)\n",
            "  Downloading pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m662.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr->voxel51-eta<0.10,>=0.9->fiftyone)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting brotli>=1.0.9 (from py7zr->voxel51-eta<0.10,>=0.9->fiftyone)\n",
            "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inflate64>=0.3.1 (from py7zr->voxel51-eta<0.10,>=0.9->fiftyone)\n",
            "  Downloading inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->voxel51-eta<0.10,>=0.9->fiftyone) (2.0.12)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal->voxel51-eta<0.10,>=0.9->fiftyone) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal->voxel51-eta<0.10,>=0.9->fiftyone) (2023.3)\n",
            "Installing collected packages: texttable, sseclient-py, rarfile, pprintpp, kaleido, brotli, xmltodict, retrying, pyzstd, pyppmd, pycryptodomex, pybcj, priority, multivolumefile, jsonlines, jmespath, inflate64, hyperframe, hpack, h11, graphql-core, ftfy, fiftyone-db, dnspython, dill, Deprecated, dacite, argcomplete, aiofiles, wsproto, strawberry-graphql, starlette, pymongo, py7zr, httpcore, h2, eventlet, botocore, voxel51-eta, sse-starlette, s3transfer, motor, mongoengine, hypercorn, httpx, fiftyone-brain, universal-analytics-python3, boto3, fiftyone\n",
            "Successfully installed Deprecated-1.2.13 aiofiles-23.1.0 argcomplete-3.0.8 boto3-1.26.139 botocore-1.29.139 brotli-1.0.9 dacite-1.7.0 dill-0.3.6 dnspython-2.3.0 eventlet-0.33.3 fiftyone-0.20.1 fiftyone-brain-0.11.0 fiftyone-db-0.4.0 ftfy-6.1.1 graphql-core-3.2.3 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-0.17.2 httpx-0.24.1 hypercorn-0.14.3 hyperframe-6.0.1 inflate64-0.3.1 jmespath-1.0.1 jsonlines-3.1.0 kaleido-0.2.1 mongoengine-0.24.2 motor-3.1.2 multivolumefile-0.2.3 pprintpp-0.4.0 priority-2.0.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pymongo-4.3.3 pyppmd-1.0.0 pyzstd-0.15.7 rarfile-4.0 retrying-1.3.4 s3transfer-0.6.1 sse-starlette-0.10.3 sseclient-py-1.7.2 starlette-0.20.4 strawberry-graphql-0.138.1 texttable-1.6.7 universal-analytics-python3-1.1.1 voxel51-eta-0.9.0 wsproto-1.2.0 xmltodict-0.13.0\n",
            "Migrating database to v0.20.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.migrations.runner:Migrating database to v0.20.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-ayx1zc1i\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-ayx1zc1i\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 3c7bb714795edc7a96c9a1a6dd83663ecd293e36\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (8.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.8.10)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.65.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.12.2)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf>=2.1 (from detectron2==0.6)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black (from detectron2==0.6)\n",
            "  Downloading black-23.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.3)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.11.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (3.3.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.40.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=7802723 sha256=40b3febf5239c4e3ed4b2fdcb3908283b745a44c4bfe3f9492377e6d53eba0ce\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yp_byqv5/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61405 sha256=fad6ed7af16af58488555a00168e419c7af513ff531155c0d6a88ba73218cf94\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=74fd5144f7229c7045c1923a8c833f6958d391c61b2c061df77a425d49725cd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-23.3.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.11.1 portalocker-2.7.0 yacs-0.1.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting umap-learn>=0.5\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5) (1.10.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5) (4.65.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn>=0.5) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn>=0.5) (67.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn>=0.5) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn>=0.5) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82816 sha256=df354068cb1831e65a8f22fff76495d1201057eff5a3f6998d67836ae0127ec4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55622 sha256=9ea50a74f0d80b6ee1998dd5ed1f2d380d25444a82a766e59b8c7e69622b8980\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.10 umap-learn-0.5.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp310-cp310-linux_x86_64.whl size=44090 sha256=259c13fdb7e16b0843285edeafb5d18cc94e7e64317c79679fa0b07cd44691e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/83/31/975b737609aba39a4099d471d5684141c1fdc3404f97e7f68a\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask 2022.12.1 requires pyyaml>=5.3.1, but you have pyyaml 5.1 which is incompatible.\n",
            "flax 0.6.9 requires PyYAML>=5.4.1, but you have pyyaml 5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyyaml-5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import random\n",
        "from torch import nn\n",
        "try:\n",
        "  from torchsummary import summary\n",
        "except:\n",
        "  !pip install torchsummary\n",
        "  from torchsummary import summary\n",
        "\n",
        "try:\n",
        "  from torchmetrics import Accuracy\n",
        "except:\n",
        "  !pip install torchmetrics\n",
        "  from torchmetrics import Accuracy\n",
        "\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#new \n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "try:\n",
        "  import fiftyone as fo\n",
        "except:\n",
        "  !pip install fiftyone\n",
        "  import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from fiftyone import ViewField as F\n",
        "import fiftyone.utils.coco as fouc\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from torchvision.transforms import functional as func\n",
        "import fiftyone.brain as fob\n",
        "import eta.core.utils as etau\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from google.colab import files,drive\n",
        "import fiftyone.utils.random as four\n",
        "from google.colab import auth\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "import fiftyone.utils.annotations as foua\n",
        "import cv2\n",
        "import os,distutils\n",
        "\n",
        "try:\n",
        "  import detectron2\n",
        "except:\n",
        "  # !git clone 'https://github.com/facebookresearch/detectron2'\n",
        "  # dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "  # !python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "  # sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "  !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "  import detectron2\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "# auth.authenticate_user()\n",
        "!pip install \"umap-learn>=0.5\"\n",
        "!pip install pyyaml==5.1\n",
        "from detectron2.engine import DefaultTrainer\n",
        "import json\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.modeling import build_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FHb4VURPZpJg"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "download the helper functions\n",
        "'''\n",
        "helper_file = \"helper_funs.py\"\n",
        "field_file = \"field_names.py\"\n",
        "\n",
        "helper_functions = requests.get(\"https://raw.githubusercontent.com/NoumanAhmad448/deep_learning_pytorch_python_computer_vision/master/helper_funs.py\")\n",
        "with open(helper_file, \"wb\") as module:\n",
        "  module.write(helper_functions.content)\n",
        "\n",
        "from helper_funs import *\n",
        "# ******************************************\n",
        "helper_functions = requests.get(\"https://raw.githubusercontent.com/NoumanAhmad448/deep_learning_pytorch_python_computer_vision/master/field_names.py\")\n",
        "with open(field_file, \"wb\") as module:\n",
        "  module.write(helper_functions.content)\n",
        "\n",
        "from field_names import *\n",
        "# ***************\n",
        "dense_file = \"dense_file.py\"\n",
        "dense_pooling_fun = requests.get(\"https://raw.githubusercontent.com/NoumanAhmad448/deep_learning_pytorch_python_computer_vision/master/dense_pooling.py\")\n",
        "with open(dense_file, \"wb\") as module:\n",
        "  module.write(dense_pooling_fun.content)\n",
        "\n",
        "from dense_file import dense_pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5qSVHwViYYi"
      },
      "outputs": [],
      "source": [
        "# Helpful resource to download COCO dataset\n",
        "# it will download the images in the director mentioned below and according to COCO dataset\n",
        "# resource, currently it is recommended way to download the COCO dataset\n",
        "\n",
        "dataset = fo.Dataset.from_dir(\"gdrive/MyDrive/data/coco-17/person\",fo.types.FiftyOneDataset,shuffle=True,seed=51,\n",
        "                               name=\"coco-178\"),split=\"train\",class=[\"person\"])\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-LhkZgBIA2X",
        "outputId": "2bc41d27-c989-42e1-827d-e95c817fecd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n",
            " 100% |███████████████| 7000/7000 [2.3m elapsed, 0s remaining, 63.4 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 7000/7000 [2.3m elapsed, 0s remaining, 63.4 samples/s]      \n"
          ]
        }
      ],
      "source": [
        "# exporting dataset\n",
        "# custom function \n",
        "export_dataset(file_name=\"coco-17/person\",dataset=dataset,\n",
        "               dataset_type=fo.types.COCODetectionDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCM7xB9SI6by",
        "outputId": "1aa7a9a9-eec9-4ff2-be59-cf6f2026fa1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |███████████████| 7000/7000 [36.1s elapsed, 0s remaining, 251.9 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 7000/7000 [36.1s elapsed, 0s remaining, 251.9 samples/s]      \n"
          ]
        }
      ],
      "source": [
        "# exporting labeling\n",
        "dataset.export(\n",
        "    # labels_path=\"gdrive/MyDrive/data/coco-17/person/coco.json\",\n",
        "    labels_path=\"person/coco.json\",\n",
        "    dataset_type=fo.types.COCODetectionDataset,\n",
        "    label_field=\"ground_truth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0Bik74ncww9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2705e0f-721e-4a83-9221-49b46ae2c044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "# You may use the functionality to load dataset directly from Google Drive\n",
        "mount_google_drive()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"coco_train5\"\n",
        "model_config = \"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"\n",
        "ds_path=\"gdrive/MyDrive/data/coco-17/person/\"\n",
        "output_dir = \"output_17_2\""
      ],
      "metadata": {
        "id": "ZLfcDqZpZonb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iU-Q2Uyu_1ma"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(model_config))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "# to train uncomment the below \n",
        "# to test comment\n",
        "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_config)\n",
        "\n",
        "\n",
        "cfg.DATASETS.TRAIN = (dataset_name,)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.001\n",
        "cfg.SOLVER.MAX_ITER = 100\n",
        "cfg.SOLVER.STEPS = [] \n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 32\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 91\n",
        "cfg.MODEL.DEVICE= setup_device()\n",
        "cfg.OUTPUT_DIR = ds_path+output_dir\n",
        "# to train comment the below \n",
        "# to test uncomment\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "\n",
        "# to train uncomment the below \n",
        "# to test comment\n",
        "# dense_pooling(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PIbAM2pv-urF"
      },
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(dataset_name, {}, ds_path+\"coco.json\", ds_path+\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqPYyePUDzGW",
        "outputId": "66caa6d1-0b44-430c-bffb-616e310451ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/24 09:13:36 d2.data.datasets.coco]: Loading gdrive/MyDrive/data/coco-17/person/coco.json takes 1.90 seconds.\n",
            "WARNING [05/24 09:13:36 d2.data.datasets.coco]: gdrive/MyDrive/data/coco-17/person/coco.json contains 66502 annotations, but only 66349 of them match to images in the file.\n",
            "[05/24 09:13:36 d2.data.datasets.coco]: Loaded 6982 images in COCO format from gdrive/MyDrive/data/coco-17/person/coco.json\n"
          ]
        }
      ],
      "source": [
        "dataset_dicts = DatasetCatalog.get(dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zIlbHtrEfQ-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e785652-3445-4b29-f525-f83e6ed23e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/24 09:13:43 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (6): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (7): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (8): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (9): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (10): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (11): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (12): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (13): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (14): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (15): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (16): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (17): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (18): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (19): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (20): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (21): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (22): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=92, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "WARNING [05/24 09:13:44 d2.data.datasets.coco]: gdrive/MyDrive/data/coco-17/person/coco.json contains 66502 annotations, but only 66349 of them match to images in the file.\n",
            "[05/24 09:13:44 d2.data.datasets.coco]: Loaded 6982 images in COCO format from gdrive/MyDrive/data/coco-17/person/coco.json\n",
            "[05/24 09:13:44 d2.data.build]: Removed 0 images with no usable annotations. 6982 images left.\n",
            "[05/24 09:13:44 d2.data.build]: Distribution of instances among all 80 categories:\n",
            "|   category    | #instances   |   category    | #instances   |   category    | #instances   |\n",
            "|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|\n",
            "|   airplane    | 237          |     apple     | 165          |   backpack    | 930          |\n",
            "|    banana     | 526          | baseball bat  | 323          | baseball gl.. | 359          |\n",
            "|     bear      | 16           |      bed      | 166          |     bench     | 820          |\n",
            "|    bicycle    | 673          |     bird      | 348          |     boat      | 665          |\n",
            "|     book      | 934          |    bottle     | 1390         |     bowl      | 699          |\n",
            "|   broccoli    | 104          |      bus      | 523          |     cake      | 339          |\n",
            "|      car      | 3462         |    carrot     | 156          |      cat      | 140          |\n",
            "|  cell phone   | 534          |     chair     | 2794         |     clock     | 305          |\n",
            "|     couch     | 311          |      cow      | 191          |      cup      | 1283         |\n",
            "| dining table  | 833          |      dog      | 342          |     donut     | 368          |\n",
            "|   elephant    | 206          | fire hydrant  | 77           |     fork      | 340          |\n",
            "|    frisbee    | 242          |    giraffe    | 97           |  hair drier   | 9            |\n",
            "|    handbag    | 1321         |     horse     | 428          |    hot dog    | 134          |\n",
            "|   keyboard    | 86           |     kite      | 972          |     knife     | 422          |\n",
            "|    laptop     | 297          |   microwave   | 67           |  motorcycle   | 807          |\n",
            "|     mouse     | 69           |    orange     | 185          |     oven      | 132          |\n",
            "| parking meter | 58           |    person     | 28385        |     pizza     | 295          |\n",
            "| potted plant  | 382          | refrigerator  | 115          |    remote     | 462          |\n",
            "|   sandwich    | 170          |   scissors    | 78           |     sheep     | 264          |\n",
            "|     sink      | 127          |  skateboard   | 601          |     skis      | 709          |\n",
            "|   snowboard   | 230          |     spoon     | 349          |  sports ball  | 654          |\n",
            "|   stop sign   | 85           |   suitcase    | 485          |   surfboard   | 669          |\n",
            "|  teddy bear   | 167          | tennis racket | 563          |      tie      | 637          |\n",
            "|    toaster    | 6            |    toilet     | 51           |  toothbrush   | 87           |\n",
            "| traffic light | 898          |     train     | 206          |     truck     | 739          |\n",
            "|      tv       | 249          |   umbrella    | 1086         |     vase      | 178          |\n",
            "|  wine glass   | 635          |     zebra     | 23           |               |              |\n",
            "|     total     | 65470        |               |              |               |              |\n",
            "[05/24 09:13:44 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[05/24 09:13:44 d2.data.build]: Using training sampler TrainingSampler\n",
            "[05/24 09:13:44 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/24 09:13:44 d2.data.common]: Serializing 6982 elements to byte tensors and concatenating them all ...\n",
            "[05/24 09:13:45 d2.data.common]: Serialized dataset takes 5.15 MiB\n",
            "[05/24 09:13:45 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_101_FPN_3x/137851257/model_final_f6e8b1.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_f6e8b1.pkl: 243MB [00:01, 195MB/s]                           \n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (92, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (92,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (364, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (364,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n"
          ]
        }
      ],
      "source": [
        "# model training\n",
        "os.makedirs(ds_path+output_dir, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lowGcPOXuExg",
        "outputId": "db472a7f-2a26-4fda-d4cd-249363a1e122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/24 09:13:46 d2.engine.train_loop]: Starting training from iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/24 09:14:31 d2.utils.events]:  eta: 0:00:49  iter: 19  total_loss: 5.299  loss_cls: 4.253  loss_box_reg: 0.9002  loss_rpn_cls: 0.03503  loss_rpn_loc: 0.08406    time: 0.6185  last_time: 0.7078  data_time: 1.2231  last_data_time: 0.0248   lr: 0.00019081  max_mem: 3626M\n",
            "[05/24 09:14:49 d2.utils.events]:  eta: 0:00:37  iter: 39  total_loss: 2.724  loss_cls: 1.735  loss_box_reg: 0.869  loss_rpn_cls: 0.02649  loss_rpn_loc: 0.06537    time: 0.6931  last_time: 0.4794  data_time: 0.0098  last_data_time: 0.0138   lr: 0.00039061  max_mem: 3627M\n",
            "[05/24 09:15:01 d2.utils.events]:  eta: 0:00:25  iter: 59  total_loss: 2.384  loss_cls: 1.337  loss_box_reg: 0.9117  loss_rpn_cls: 0.025  loss_rpn_loc: 0.05766    time: 0.6635  last_time: 0.7048  data_time: 0.0080  last_data_time: 0.0247   lr: 0.00059041  max_mem: 3627M\n",
            "[05/24 09:15:13 d2.utils.events]:  eta: 0:00:12  iter: 79  total_loss: 1.941  loss_cls: 1.054  loss_box_reg: 0.7936  loss_rpn_cls: 0.02931  loss_rpn_loc: 0.07067    time: 0.6496  last_time: 0.6536  data_time: 0.0069  last_data_time: 0.0057   lr: 0.00079021  max_mem: 3627M\n",
            "[05/24 09:15:28 d2.utils.events]:  eta: 0:00:00  iter: 99  total_loss: 1.676  loss_cls: 0.8196  loss_box_reg: 0.7823  loss_rpn_cls: 0.02169  loss_rpn_loc: 0.04494    time: 0.6533  last_time: 0.5812  data_time: 0.0144  last_data_time: 0.0062   lr: 0.00099001  max_mem: 3627M\n",
            "[05/24 09:15:28 d2.engine.hooks]: Overall training speed: 98 iterations in 0:01:04 (0.6533 s / it)\n",
            "[05/24 09:15:28 d2.engine.hooks]: Total training time: 0:01:08 (0:00:04 on hooks)\n"
          ]
        }
      ],
      "source": [
        "#train model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Model with dense pooling**"
      ],
      "metadata": {
        "id": "1TMRwUzLC9Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"output_17_3\""
      ],
      "metadata": {
        "id": "ehga755tCvde"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = get_cfg()\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(model_config))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "# to train uncomment the below \n",
        "# to test comment\n",
        "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_config)\n",
        "\n",
        "\n",
        "cfg.DATASETS.TRAIN = (dataset_name,)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.001\n",
        "cfg.SOLVER.MAX_ITER = 100\n",
        "cfg.SOLVER.STEPS = [] \n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 32\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 91\n",
        "cfg.MODEL.DEVICE= setup_device()\n",
        "cfg.OUTPUT_DIR = ds_path+output_dir\n",
        "# to train comment the below \n",
        "# to test uncomment\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "\n",
        "# to train uncomment the below \n",
        "# to test comment\n",
        "dense_pooling(cfg)"
      ],
      "metadata": {
        "id": "jfnUQz2pCc1-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "os.makedirs(ds_path+output_dir, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zz-mnBFDMtc",
        "outputId": "2432837f-5508-4602-f395-5cc75029cc18"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/24 09:17:54 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (6): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (7): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (8): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (9): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (10): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (11): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (12): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (13): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (14): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (15): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (16): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (17): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (18): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (19): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (20): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (21): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (22): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=92, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "WARNING [05/24 09:17:54 d2.data.datasets.coco]: gdrive/MyDrive/data/coco-17/person/coco.json contains 66502 annotations, but only 66349 of them match to images in the file.\n",
            "[05/24 09:17:54 d2.data.datasets.coco]: Loaded 6982 images in COCO format from gdrive/MyDrive/data/coco-17/person/coco.json\n",
            "[05/24 09:17:54 d2.data.build]: Removed 0 images with no usable annotations. 6982 images left.\n",
            "[05/24 09:17:55 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[05/24 09:17:55 d2.data.build]: Using training sampler TrainingSampler\n",
            "[05/24 09:17:55 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/24 09:17:55 d2.data.common]: Serializing 6982 elements to byte tensors and concatenating them all ...\n",
            "[05/24 09:17:55 d2.data.common]: Serialized dataset takes 5.15 MiB\n",
            "[05/24 09:17:55 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_101_FPN_3x/137851257/model_final_f6e8b1.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (92, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (92,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (364, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (364,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/24 09:17:55 d2.engine.train_loop]: Starting training from iteration 0\n",
            "[05/24 09:18:08 d2.utils.events]:  eta: 0:00:48  iter: 19  total_loss: 5.184  loss_cls: 4.217  loss_box_reg: 0.9158  loss_rpn_cls: 0.01771  loss_rpn_loc: 0.05898    time: 0.6014  last_time: 0.6242  data_time: 0.0414  last_data_time: 0.0102   lr: 0.00019081  max_mem: 3627M\n",
            "[05/24 09:18:19 d2.utils.events]:  eta: 0:00:36  iter: 39  total_loss: 2.665  loss_cls: 1.596  loss_box_reg: 0.924  loss_rpn_cls: 0.02069  loss_rpn_loc: 0.03327    time: 0.5953  last_time: 0.6047  data_time: 0.0083  last_data_time: 0.0078   lr: 0.00039061  max_mem: 3627M\n",
            "[05/24 09:18:32 d2.utils.events]:  eta: 0:00:24  iter: 59  total_loss: 2.277  loss_cls: 1.301  loss_box_reg: 0.8671  loss_rpn_cls: 0.0297  loss_rpn_loc: 0.07535    time: 0.6061  last_time: 0.6324  data_time: 0.0109  last_data_time: 0.0108   lr: 0.00059041  max_mem: 3627M\n",
            "[05/24 09:18:45 d2.utils.events]:  eta: 0:00:12  iter: 79  total_loss: 2.038  loss_cls: 1.055  loss_box_reg: 0.8343  loss_rpn_cls: 0.02088  loss_rpn_loc: 0.0574    time: 0.6126  last_time: 0.6778  data_time: 0.0101  last_data_time: 0.0061   lr: 0.00079021  max_mem: 3627M\n",
            "[05/24 09:19:02 d2.utils.events]:  eta: 0:00:00  iter: 99  total_loss: 1.813  loss_cls: 0.9071  loss_box_reg: 0.8182  loss_rpn_cls: 0.0188  loss_rpn_loc: 0.03865    time: 0.6100  last_time: 0.8512  data_time: 0.0094  last_data_time: 0.0451   lr: 0.00099001  max_mem: 3627M\n",
            "[05/24 09:19:02 d2.engine.hooks]: Overall training speed: 98 iterations in 0:00:59 (0.6100 s / it)\n",
            "[05/24 09:19:02 d2.engine.hooks]: Total training time: 0:01:05 (0:00:05 on hooks)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "\n",
        "# Prediction Without Dense Pooling\n",
        "predictor = DefaultPredictor(cfg)\n",
        "evaluator = COCOEvaluator(dataset_name, output_dir=cfg.OUTPUT_DIR)\n",
        "val_loader = build_detection_test_loader(cfg, dataset_name)\n",
        "results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZP1v6bjGYT6",
        "outputId": "1f5f7cd2-15a6-4a44-c6b0-8979051540f9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/24 09:19:03 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from gdrive/MyDrive/data/coco-17/person/output_17_2/model_final.pth ...\n",
            "WARNING [05/24 09:19:05 d2.data.datasets.coco]: gdrive/MyDrive/data/coco-17/person/coco.json contains 66502 annotations, but only 66349 of them match to images in the file.\n",
            "[05/24 09:19:05 d2.data.datasets.coco]: Loaded 6982 images in COCO format from gdrive/MyDrive/data/coco-17/person/coco.json\n",
            "[05/24 09:19:06 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/24 09:19:06 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/24 09:19:06 d2.data.common]: Serializing 6982 elements to byte tensors and concatenating them all ...\n",
            "[05/24 09:19:06 d2.data.common]: Serialized dataset takes 5.15 MiB\n",
            "[05/24 09:19:06 d2.evaluation.evaluator]: Start inference on 6982 batches\n",
            "[05/24 09:19:08 d2.evaluation.evaluator]: Inference done 11/6982. Dataloading: 0.0013 s/iter. Inference: 0.1404 s/iter. Eval: 0.0003 s/iter. Total: 0.1420 s/iter. ETA=0:16:30\n",
            "[05/24 09:19:13 d2.evaluation.evaluator]: Inference done 37/6982. Dataloading: 0.0098 s/iter. Inference: 0.1788 s/iter. Eval: 0.0016 s/iter. Total: 0.1902 s/iter. ETA=0:22:01\n",
            "[05/24 09:19:18 d2.evaluation.evaluator]: Inference done 55/6982. Dataloading: 0.0097 s/iter. Inference: 0.2126 s/iter. Eval: 0.0016 s/iter. Total: 0.2247 s/iter. ETA=0:25:56\n",
            "[05/24 09:19:23 d2.evaluation.evaluator]: Inference done 90/6982. Dataloading: 0.0065 s/iter. Inference: 0.1832 s/iter. Eval: 0.0010 s/iter. Total: 0.1913 s/iter. ETA=0:21:58\n",
            "[05/24 09:19:28 d2.evaluation.evaluator]: Inference done 125/6982. Dataloading: 0.0052 s/iter. Inference: 0.1708 s/iter. Eval: 0.0008 s/iter. Total: 0.1773 s/iter. ETA=0:20:15\n",
            "[05/24 09:19:33 d2.evaluation.evaluator]: Inference done 152/6982. Dataloading: 0.0053 s/iter. Inference: 0.1725 s/iter. Eval: 0.0011 s/iter. Total: 0.1793 s/iter. ETA=0:20:24\n",
            "[05/24 09:19:38 d2.evaluation.evaluator]: Inference done 183/6982. Dataloading: 0.0067 s/iter. Inference: 0.1682 s/iter. Eval: 0.0010 s/iter. Total: 0.1763 s/iter. ETA=0:19:58\n",
            "[05/24 09:19:44 d2.evaluation.evaluator]: Inference done 217/6982. Dataloading: 0.0063 s/iter. Inference: 0.1648 s/iter. Eval: 0.0009 s/iter. Total: 0.1723 s/iter. ETA=0:19:25\n",
            "[05/24 09:19:49 d2.evaluation.evaluator]: Inference done 248/6982. Dataloading: 0.0069 s/iter. Inference: 0.1629 s/iter. Eval: 0.0008 s/iter. Total: 0.1709 s/iter. ETA=0:19:10\n",
            "[05/24 09:19:54 d2.evaluation.evaluator]: Inference done 279/6982. Dataloading: 0.0073 s/iter. Inference: 0.1618 s/iter. Eval: 0.0008 s/iter. Total: 0.1701 s/iter. ETA=0:19:00\n",
            "[05/24 09:19:59 d2.evaluation.evaluator]: Inference done 314/6982. Dataloading: 0.0067 s/iter. Inference: 0.1596 s/iter. Eval: 0.0007 s/iter. Total: 0.1673 s/iter. ETA=0:18:35\n",
            "[05/24 09:20:04 d2.evaluation.evaluator]: Inference done 348/6982. Dataloading: 0.0063 s/iter. Inference: 0.1581 s/iter. Eval: 0.0007 s/iter. Total: 0.1653 s/iter. ETA=0:18:16\n",
            "[05/24 09:20:09 d2.evaluation.evaluator]: Inference done 381/6982. Dataloading: 0.0060 s/iter. Inference: 0.1575 s/iter. Eval: 0.0007 s/iter. Total: 0.1644 s/iter. ETA=0:18:05\n",
            "[05/24 09:20:14 d2.evaluation.evaluator]: Inference done 416/6982. Dataloading: 0.0059 s/iter. Inference: 0.1561 s/iter. Eval: 0.0007 s/iter. Total: 0.1629 s/iter. ETA=0:17:49\n",
            "[05/24 09:20:19 d2.evaluation.evaluator]: Inference done 451/6982. Dataloading: 0.0056 s/iter. Inference: 0.1550 s/iter. Eval: 0.0006 s/iter. Total: 0.1614 s/iter. ETA=0:17:34\n",
            "[05/24 09:20:24 d2.evaluation.evaluator]: Inference done 484/6982. Dataloading: 0.0054 s/iter. Inference: 0.1547 s/iter. Eval: 0.0006 s/iter. Total: 0.1609 s/iter. ETA=0:17:25\n",
            "[05/24 09:20:29 d2.evaluation.evaluator]: Inference done 516/6982. Dataloading: 0.0055 s/iter. Inference: 0.1543 s/iter. Eval: 0.0006 s/iter. Total: 0.1607 s/iter. ETA=0:17:18\n",
            "[05/24 09:20:34 d2.evaluation.evaluator]: Inference done 551/6982. Dataloading: 0.0053 s/iter. Inference: 0.1537 s/iter. Eval: 0.0006 s/iter. Total: 0.1597 s/iter. ETA=0:17:07\n",
            "[05/24 09:20:39 d2.evaluation.evaluator]: Inference done 584/6982. Dataloading: 0.0054 s/iter. Inference: 0.1532 s/iter. Eval: 0.0006 s/iter. Total: 0.1593 s/iter. ETA=0:16:59\n",
            "[05/24 09:20:44 d2.evaluation.evaluator]: Inference done 618/6982. Dataloading: 0.0053 s/iter. Inference: 0.1527 s/iter. Eval: 0.0005 s/iter. Total: 0.1588 s/iter. ETA=0:16:50\n",
            "[05/24 09:20:49 d2.evaluation.evaluator]: Inference done 652/6982. Dataloading: 0.0051 s/iter. Inference: 0.1523 s/iter. Eval: 0.0005 s/iter. Total: 0.1582 s/iter. ETA=0:16:41\n",
            "[05/24 09:20:55 d2.evaluation.evaluator]: Inference done 687/6982. Dataloading: 0.0050 s/iter. Inference: 0.1518 s/iter. Eval: 0.0005 s/iter. Total: 0.1575 s/iter. ETA=0:16:31\n",
            "[05/24 09:21:00 d2.evaluation.evaluator]: Inference done 712/6982. Dataloading: 0.0066 s/iter. Inference: 0.1518 s/iter. Eval: 0.0005 s/iter. Total: 0.1592 s/iter. ETA=0:16:37\n",
            "[05/24 09:21:05 d2.evaluation.evaluator]: Inference done 747/6982. Dataloading: 0.0064 s/iter. Inference: 0.1514 s/iter. Eval: 0.0005 s/iter. Total: 0.1585 s/iter. ETA=0:16:28\n",
            "[05/24 09:21:10 d2.evaluation.evaluator]: Inference done 782/6982. Dataloading: 0.0062 s/iter. Inference: 0.1510 s/iter. Eval: 0.0005 s/iter. Total: 0.1579 s/iter. ETA=0:16:18\n",
            "[05/24 09:21:15 d2.evaluation.evaluator]: Inference done 815/6982. Dataloading: 0.0061 s/iter. Inference: 0.1510 s/iter. Eval: 0.0005 s/iter. Total: 0.1578 s/iter. ETA=0:16:13\n",
            "[05/24 09:21:20 d2.evaluation.evaluator]: Inference done 848/6982. Dataloading: 0.0061 s/iter. Inference: 0.1510 s/iter. Eval: 0.0005 s/iter. Total: 0.1578 s/iter. ETA=0:16:07\n",
            "[05/24 09:21:25 d2.evaluation.evaluator]: Inference done 883/6982. Dataloading: 0.0059 s/iter. Inference: 0.1507 s/iter. Eval: 0.0005 s/iter. Total: 0.1573 s/iter. ETA=0:15:59\n",
            "[05/24 09:21:30 d2.evaluation.evaluator]: Inference done 916/6982. Dataloading: 0.0058 s/iter. Inference: 0.1506 s/iter. Eval: 0.0005 s/iter. Total: 0.1571 s/iter. ETA=0:15:53\n",
            "[05/24 09:21:35 d2.evaluation.evaluator]: Inference done 948/6982. Dataloading: 0.0058 s/iter. Inference: 0.1507 s/iter. Eval: 0.0005 s/iter. Total: 0.1572 s/iter. ETA=0:15:48\n",
            "[05/24 09:21:40 d2.evaluation.evaluator]: Inference done 983/6982. Dataloading: 0.0057 s/iter. Inference: 0.1504 s/iter. Eval: 0.0005 s/iter. Total: 0.1568 s/iter. ETA=0:15:40\n",
            "[05/24 09:21:45 d2.evaluation.evaluator]: Inference done 1017/6982. Dataloading: 0.0055 s/iter. Inference: 0.1502 s/iter. Eval: 0.0005 s/iter. Total: 0.1564 s/iter. ETA=0:15:33\n",
            "[05/24 09:21:51 d2.evaluation.evaluator]: Inference done 1050/6982. Dataloading: 0.0055 s/iter. Inference: 0.1502 s/iter. Eval: 0.0005 s/iter. Total: 0.1564 s/iter. ETA=0:15:27\n",
            "[05/24 09:21:56 d2.evaluation.evaluator]: Inference done 1084/6982. Dataloading: 0.0055 s/iter. Inference: 0.1500 s/iter. Eval: 0.0005 s/iter. Total: 0.1562 s/iter. ETA=0:15:21\n",
            "[05/24 09:22:01 d2.evaluation.evaluator]: Inference done 1119/6982. Dataloading: 0.0054 s/iter. Inference: 0.1498 s/iter. Eval: 0.0005 s/iter. Total: 0.1559 s/iter. ETA=0:15:14\n",
            "[05/24 09:22:06 d2.evaluation.evaluator]: Inference done 1151/6982. Dataloading: 0.0055 s/iter. Inference: 0.1498 s/iter. Eval: 0.0005 s/iter. Total: 0.1559 s/iter. ETA=0:15:09\n",
            "[05/24 09:22:11 d2.evaluation.evaluator]: Inference done 1182/6982. Dataloading: 0.0057 s/iter. Inference: 0.1497 s/iter. Eval: 0.0005 s/iter. Total: 0.1561 s/iter. ETA=0:15:05\n",
            "[05/24 09:22:16 d2.evaluation.evaluator]: Inference done 1217/6982. Dataloading: 0.0056 s/iter. Inference: 0.1496 s/iter. Eval: 0.0004 s/iter. Total: 0.1558 s/iter. ETA=0:14:58\n",
            "[05/24 09:22:21 d2.evaluation.evaluator]: Inference done 1251/6982. Dataloading: 0.0055 s/iter. Inference: 0.1495 s/iter. Eval: 0.0004 s/iter. Total: 0.1557 s/iter. ETA=0:14:52\n",
            "[05/24 09:22:26 d2.evaluation.evaluator]: Inference done 1284/6982. Dataloading: 0.0054 s/iter. Inference: 0.1495 s/iter. Eval: 0.0004 s/iter. Total: 0.1556 s/iter. ETA=0:14:46\n",
            "[05/24 09:22:31 d2.evaluation.evaluator]: Inference done 1318/6982. Dataloading: 0.0053 s/iter. Inference: 0.1495 s/iter. Eval: 0.0004 s/iter. Total: 0.1554 s/iter. ETA=0:14:40\n",
            "[05/24 09:22:36 d2.evaluation.evaluator]: Inference done 1353/6982. Dataloading: 0.0053 s/iter. Inference: 0.1493 s/iter. Eval: 0.0004 s/iter. Total: 0.1552 s/iter. ETA=0:14:33\n",
            "[05/24 09:22:41 d2.evaluation.evaluator]: Inference done 1385/6982. Dataloading: 0.0054 s/iter. Inference: 0.1492 s/iter. Eval: 0.0004 s/iter. Total: 0.1552 s/iter. ETA=0:14:28\n",
            "[05/24 09:22:46 d2.evaluation.evaluator]: Inference done 1417/6982. Dataloading: 0.0055 s/iter. Inference: 0.1491 s/iter. Eval: 0.0004 s/iter. Total: 0.1552 s/iter. ETA=0:14:23\n",
            "[05/24 09:22:51 d2.evaluation.evaluator]: Inference done 1452/6982. Dataloading: 0.0054 s/iter. Inference: 0.1490 s/iter. Eval: 0.0004 s/iter. Total: 0.1550 s/iter. ETA=0:14:17\n",
            "[05/24 09:22:57 d2.evaluation.evaluator]: Inference done 1485/6982. Dataloading: 0.0054 s/iter. Inference: 0.1490 s/iter. Eval: 0.0004 s/iter. Total: 0.1550 s/iter. ETA=0:14:12\n",
            "[05/24 09:23:02 d2.evaluation.evaluator]: Inference done 1517/6982. Dataloading: 0.0053 s/iter. Inference: 0.1491 s/iter. Eval: 0.0004 s/iter. Total: 0.1551 s/iter. ETA=0:14:07\n",
            "[05/24 09:23:07 d2.evaluation.evaluator]: Inference done 1551/6982. Dataloading: 0.0053 s/iter. Inference: 0.1491 s/iter. Eval: 0.0004 s/iter. Total: 0.1550 s/iter. ETA=0:14:01\n",
            "[05/24 09:23:12 d2.evaluation.evaluator]: Inference done 1582/6982. Dataloading: 0.0053 s/iter. Inference: 0.1491 s/iter. Eval: 0.0004 s/iter. Total: 0.1551 s/iter. ETA=0:13:57\n",
            "[05/24 09:23:17 d2.evaluation.evaluator]: Inference done 1613/6982. Dataloading: 0.0054 s/iter. Inference: 0.1493 s/iter. Eval: 0.0005 s/iter. Total: 0.1553 s/iter. ETA=0:13:53\n",
            "[05/24 09:23:22 d2.evaluation.evaluator]: Inference done 1646/6982. Dataloading: 0.0053 s/iter. Inference: 0.1493 s/iter. Eval: 0.0004 s/iter. Total: 0.1553 s/iter. ETA=0:13:48\n",
            "[05/24 09:23:27 d2.evaluation.evaluator]: Inference done 1681/6982. Dataloading: 0.0053 s/iter. Inference: 0.1492 s/iter. Eval: 0.0004 s/iter. Total: 0.1551 s/iter. ETA=0:13:42\n",
            "[05/24 09:23:32 d2.evaluation.evaluator]: Inference done 1714/6982. Dataloading: 0.0052 s/iter. Inference: 0.1492 s/iter. Eval: 0.0004 s/iter. Total: 0.1551 s/iter. ETA=0:13:36\n",
            "[05/24 09:23:37 d2.evaluation.evaluator]: Inference done 1746/6982. Dataloading: 0.0054 s/iter. Inference: 0.1492 s/iter. Eval: 0.0004 s/iter. Total: 0.1551 s/iter. ETA=0:13:32\n",
            "[05/24 09:23:42 d2.evaluation.evaluator]: Inference done 1780/6982. Dataloading: 0.0054 s/iter. Inference: 0.1490 s/iter. Eval: 0.0004 s/iter. Total: 0.1550 s/iter. ETA=0:13:26\n",
            "[05/24 09:23:47 d2.evaluation.evaluator]: Inference done 1814/6982. Dataloading: 0.0053 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1549 s/iter. ETA=0:13:20\n",
            "[05/24 09:23:52 d2.evaluation.evaluator]: Inference done 1846/6982. Dataloading: 0.0053 s/iter. Inference: 0.1490 s/iter. Eval: 0.0004 s/iter. Total: 0.1550 s/iter. ETA=0:13:16\n",
            "[05/24 09:23:57 d2.evaluation.evaluator]: Inference done 1880/6982. Dataloading: 0.0053 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1549 s/iter. ETA=0:13:10\n",
            "[05/24 09:24:03 d2.evaluation.evaluator]: Inference done 1914/6982. Dataloading: 0.0053 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1548 s/iter. ETA=0:13:04\n",
            "[05/24 09:24:08 d2.evaluation.evaluator]: Inference done 1946/6982. Dataloading: 0.0053 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1548 s/iter. ETA=0:12:59\n",
            "[05/24 09:24:13 d2.evaluation.evaluator]: Inference done 1979/6982. Dataloading: 0.0053 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1548 s/iter. ETA=0:12:54\n",
            "[05/24 09:24:18 d2.evaluation.evaluator]: Inference done 2013/6982. Dataloading: 0.0052 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1547 s/iter. ETA=0:12:48\n",
            "[05/24 09:24:23 d2.evaluation.evaluator]: Inference done 2047/6982. Dataloading: 0.0052 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1546 s/iter. ETA=0:12:43\n",
            "[05/24 09:24:28 d2.evaluation.evaluator]: Inference done 2079/6982. Dataloading: 0.0052 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1547 s/iter. ETA=0:12:38\n",
            "[05/24 09:24:33 d2.evaluation.evaluator]: Inference done 2114/6982. Dataloading: 0.0052 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1545 s/iter. ETA=0:12:32\n",
            "[05/24 09:24:38 d2.evaluation.evaluator]: Inference done 2148/6982. Dataloading: 0.0051 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1544 s/iter. ETA=0:12:26\n",
            "[05/24 09:24:43 d2.evaluation.evaluator]: Inference done 2180/6982. Dataloading: 0.0051 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1544 s/iter. ETA=0:12:21\n",
            "[05/24 09:24:48 d2.evaluation.evaluator]: Inference done 2212/6982. Dataloading: 0.0051 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1545 s/iter. ETA=0:12:16\n",
            "[05/24 09:24:53 d2.evaluation.evaluator]: Inference done 2246/6982. Dataloading: 0.0051 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1544 s/iter. ETA=0:12:11\n",
            "[05/24 09:24:58 d2.evaluation.evaluator]: Inference done 2279/6982. Dataloading: 0.0051 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1544 s/iter. ETA=0:12:06\n",
            "[05/24 09:25:03 d2.evaluation.evaluator]: Inference done 2310/6982. Dataloading: 0.0051 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1545 s/iter. ETA=0:12:01\n",
            "[05/24 09:25:08 d2.evaluation.evaluator]: Inference done 2344/6982. Dataloading: 0.0050 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1544 s/iter. ETA=0:11:56\n",
            "[05/24 09:25:13 d2.evaluation.evaluator]: Inference done 2378/6982. Dataloading: 0.0050 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1543 s/iter. ETA=0:11:50\n",
            "[05/24 09:25:18 d2.evaluation.evaluator]: Inference done 2411/6982. Dataloading: 0.0050 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1544 s/iter. ETA=0:11:45\n",
            "[05/24 09:25:24 d2.evaluation.evaluator]: Inference done 2444/6982. Dataloading: 0.0050 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1543 s/iter. ETA=0:11:40\n",
            "[05/24 09:25:29 d2.evaluation.evaluator]: Inference done 2478/6982. Dataloading: 0.0049 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1543 s/iter. ETA=0:11:34\n",
            "[05/24 09:25:34 d2.evaluation.evaluator]: Inference done 2511/6982. Dataloading: 0.0049 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1543 s/iter. ETA=0:11:29\n",
            "[05/24 09:25:39 d2.evaluation.evaluator]: Inference done 2543/6982. Dataloading: 0.0049 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1543 s/iter. ETA=0:11:25\n",
            "[05/24 09:25:44 d2.evaluation.evaluator]: Inference done 2577/6982. Dataloading: 0.0049 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1543 s/iter. ETA=0:11:19\n",
            "[05/24 09:25:49 d2.evaluation.evaluator]: Inference done 2611/6982. Dataloading: 0.0048 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1542 s/iter. ETA=0:11:13\n",
            "[05/24 09:25:54 d2.evaluation.evaluator]: Inference done 2642/6982. Dataloading: 0.0049 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1543 s/iter. ETA=0:11:09\n",
            "[05/24 09:25:59 d2.evaluation.evaluator]: Inference done 2676/6982. Dataloading: 0.0049 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1543 s/iter. ETA=0:11:04\n",
            "[05/24 09:26:04 d2.evaluation.evaluator]: Inference done 2711/6982. Dataloading: 0.0049 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1542 s/iter. ETA=0:10:58\n",
            "[05/24 09:26:09 d2.evaluation.evaluator]: Inference done 2743/6982. Dataloading: 0.0048 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1542 s/iter. ETA=0:10:53\n",
            "[05/24 09:26:14 d2.evaluation.evaluator]: Inference done 2777/6982. Dataloading: 0.0048 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:10:48\n",
            "[05/24 09:26:19 d2.evaluation.evaluator]: Inference done 2811/6982. Dataloading: 0.0048 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:10:42\n",
            "[05/24 09:26:24 d2.evaluation.evaluator]: Inference done 2844/6982. Dataloading: 0.0048 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:10:37\n",
            "[05/24 09:26:30 d2.evaluation.evaluator]: Inference done 2875/6982. Dataloading: 0.0048 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1542 s/iter. ETA=0:10:33\n",
            "[05/24 09:26:35 d2.evaluation.evaluator]: Inference done 2910/6982. Dataloading: 0.0047 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:10:27\n",
            "[05/24 09:26:40 d2.evaluation.evaluator]: Inference done 2944/6982. Dataloading: 0.0047 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:10:21\n",
            "[05/24 09:26:45 d2.evaluation.evaluator]: Inference done 2976/6982. Dataloading: 0.0047 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:10:17\n",
            "[05/24 09:26:50 d2.evaluation.evaluator]: Inference done 3010/6982. Dataloading: 0.0047 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:10:11\n",
            "[05/24 09:26:55 d2.evaluation.evaluator]: Inference done 3044/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:10:06\n",
            "[05/24 09:27:00 d2.evaluation.evaluator]: Inference done 3078/6982. Dataloading: 0.0046 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:10:00\n",
            "[05/24 09:27:05 d2.evaluation.evaluator]: Inference done 3111/6982. Dataloading: 0.0046 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:09:55\n",
            "[05/24 09:27:10 d2.evaluation.evaluator]: Inference done 3145/6982. Dataloading: 0.0046 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:09:50\n",
            "[05/24 09:27:15 d2.evaluation.evaluator]: Inference done 3179/6982. Dataloading: 0.0046 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1538 s/iter. ETA=0:09:44\n",
            "[05/24 09:27:20 d2.evaluation.evaluator]: Inference done 3212/6982. Dataloading: 0.0046 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1538 s/iter. ETA=0:09:39\n",
            "[05/24 09:27:25 d2.evaluation.evaluator]: Inference done 3246/6982. Dataloading: 0.0046 s/iter. Inference: 0.1486 s/iter. Eval: 0.0004 s/iter. Total: 0.1538 s/iter. ETA=0:09:34\n",
            "[05/24 09:27:30 d2.evaluation.evaluator]: Inference done 3280/6982. Dataloading: 0.0045 s/iter. Inference: 0.1486 s/iter. Eval: 0.0004 s/iter. Total: 0.1537 s/iter. ETA=0:09:29\n",
            "[05/24 09:27:36 d2.evaluation.evaluator]: Inference done 3312/6982. Dataloading: 0.0046 s/iter. Inference: 0.1486 s/iter. Eval: 0.0004 s/iter. Total: 0.1538 s/iter. ETA=0:09:24\n",
            "[05/24 09:27:41 d2.evaluation.evaluator]: Inference done 3344/6982. Dataloading: 0.0046 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1538 s/iter. ETA=0:09:19\n",
            "[05/24 09:27:46 d2.evaluation.evaluator]: Inference done 3378/6982. Dataloading: 0.0046 s/iter. Inference: 0.1486 s/iter. Eval: 0.0004 s/iter. Total: 0.1538 s/iter. ETA=0:09:14\n",
            "[05/24 09:27:51 d2.evaluation.evaluator]: Inference done 3412/6982. Dataloading: 0.0045 s/iter. Inference: 0.1486 s/iter. Eval: 0.0004 s/iter. Total: 0.1537 s/iter. ETA=0:09:08\n",
            "[05/24 09:27:56 d2.evaluation.evaluator]: Inference done 3444/6982. Dataloading: 0.0045 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1538 s/iter. ETA=0:09:04\n",
            "[05/24 09:28:01 d2.evaluation.evaluator]: Inference done 3475/6982. Dataloading: 0.0046 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:08:59\n",
            "[05/24 09:28:06 d2.evaluation.evaluator]: Inference done 3508/6982. Dataloading: 0.0046 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:08:54\n",
            "[05/24 09:28:11 d2.evaluation.evaluator]: Inference done 3541/6982. Dataloading: 0.0046 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:08:49\n",
            "[05/24 09:28:16 d2.evaluation.evaluator]: Inference done 3570/6982. Dataloading: 0.0047 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:08:45\n",
            "[05/24 09:28:21 d2.evaluation.evaluator]: Inference done 3604/6982. Dataloading: 0.0047 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:08:40\n",
            "[05/24 09:28:26 d2.evaluation.evaluator]: Inference done 3638/6982. Dataloading: 0.0047 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:08:34\n",
            "[05/24 09:28:32 d2.evaluation.evaluator]: Inference done 3671/6982. Dataloading: 0.0047 s/iter. Inference: 0.1487 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:08:29\n",
            "[05/24 09:28:37 d2.evaluation.evaluator]: Inference done 3701/6982. Dataloading: 0.0047 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:08:25\n",
            "[05/24 09:28:42 d2.evaluation.evaluator]: Inference done 3735/6982. Dataloading: 0.0047 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:08:20\n",
            "[05/24 09:28:47 d2.evaluation.evaluator]: Inference done 3767/6982. Dataloading: 0.0047 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:08:15\n",
            "[05/24 09:28:52 d2.evaluation.evaluator]: Inference done 3800/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:08:10\n",
            "[05/24 09:28:57 d2.evaluation.evaluator]: Inference done 3835/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:08:04\n",
            "[05/24 09:29:02 d2.evaluation.evaluator]: Inference done 3868/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:07:59\n",
            "[05/24 09:29:07 d2.evaluation.evaluator]: Inference done 3900/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:07:54\n",
            "[05/24 09:29:12 d2.evaluation.evaluator]: Inference done 3934/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:07:49\n",
            "[05/24 09:29:17 d2.evaluation.evaluator]: Inference done 3968/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:07:44\n",
            "[05/24 09:29:22 d2.evaluation.evaluator]: Inference done 4000/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:07:39\n",
            "[05/24 09:29:27 d2.evaluation.evaluator]: Inference done 4034/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:07:33\n",
            "[05/24 09:29:33 d2.evaluation.evaluator]: Inference done 4068/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:07:28\n",
            "[05/24 09:29:38 d2.evaluation.evaluator]: Inference done 4100/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:07:23\n",
            "[05/24 09:29:43 d2.evaluation.evaluator]: Inference done 4134/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:07:18\n",
            "[05/24 09:29:48 d2.evaluation.evaluator]: Inference done 4168/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:07:13\n",
            "[05/24 09:29:53 d2.evaluation.evaluator]: Inference done 4202/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:07:07\n",
            "[05/24 09:29:58 d2.evaluation.evaluator]: Inference done 4234/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:07:03\n",
            "[05/24 09:30:03 d2.evaluation.evaluator]: Inference done 4264/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:06:58\n",
            "[05/24 09:30:08 d2.evaluation.evaluator]: Inference done 4298/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:06:53\n",
            "[05/24 09:30:13 d2.evaluation.evaluator]: Inference done 4330/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:06:48\n",
            "[05/24 09:30:18 d2.evaluation.evaluator]: Inference done 4363/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:06:43\n",
            "[05/24 09:30:23 d2.evaluation.evaluator]: Inference done 4397/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:06:38\n",
            "[05/24 09:30:28 d2.evaluation.evaluator]: Inference done 4430/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:06:32\n",
            "[05/24 09:30:34 d2.evaluation.evaluator]: Inference done 4463/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:06:27\n",
            "[05/24 09:30:39 d2.evaluation.evaluator]: Inference done 4496/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:06:22\n",
            "[05/24 09:30:44 d2.evaluation.evaluator]: Inference done 4528/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:06:17\n",
            "[05/24 09:30:49 d2.evaluation.evaluator]: Inference done 4560/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:06:13\n",
            "[05/24 09:30:54 d2.evaluation.evaluator]: Inference done 4593/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:06:07\n",
            "[05/24 09:30:59 d2.evaluation.evaluator]: Inference done 4627/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:06:02\n",
            "[05/24 09:31:04 d2.evaluation.evaluator]: Inference done 4660/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:05:57\n",
            "[05/24 09:31:09 d2.evaluation.evaluator]: Inference done 4692/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:05:52\n",
            "[05/24 09:31:14 d2.evaluation.evaluator]: Inference done 4726/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:05:47\n",
            "[05/24 09:31:19 d2.evaluation.evaluator]: Inference done 4761/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:05:41\n",
            "[05/24 09:31:24 d2.evaluation.evaluator]: Inference done 4793/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:05:37\n",
            "[05/24 09:31:29 d2.evaluation.evaluator]: Inference done 4826/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:05:31\n",
            "[05/24 09:31:35 d2.evaluation.evaluator]: Inference done 4860/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:05:26\n",
            "[05/24 09:31:40 d2.evaluation.evaluator]: Inference done 4891/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:05:22\n",
            "[05/24 09:31:45 d2.evaluation.evaluator]: Inference done 4923/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:05:17\n",
            "[05/24 09:31:50 d2.evaluation.evaluator]: Inference done 4957/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:05:11\n",
            "[05/24 09:31:55 d2.evaluation.evaluator]: Inference done 4989/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:05:06\n",
            "[05/24 09:32:00 d2.evaluation.evaluator]: Inference done 5021/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:05:02\n",
            "[05/24 09:32:05 d2.evaluation.evaluator]: Inference done 5054/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:04:56\n",
            "[05/24 09:32:10 d2.evaluation.evaluator]: Inference done 5088/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:04:51\n",
            "[05/24 09:32:15 d2.evaluation.evaluator]: Inference done 5120/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:04:46\n",
            "[05/24 09:32:20 d2.evaluation.evaluator]: Inference done 5154/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:04:41\n",
            "[05/24 09:32:25 d2.evaluation.evaluator]: Inference done 5188/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:04:36\n",
            "[05/24 09:32:30 d2.evaluation.evaluator]: Inference done 5221/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:04:31\n",
            "[05/24 09:32:35 d2.evaluation.evaluator]: Inference done 5252/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:04:26\n",
            "[05/24 09:32:40 d2.evaluation.evaluator]: Inference done 5286/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:04:21\n",
            "[05/24 09:32:45 d2.evaluation.evaluator]: Inference done 5320/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:04:15\n",
            "[05/24 09:32:50 d2.evaluation.evaluator]: Inference done 5352/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:04:10\n",
            "[05/24 09:32:56 d2.evaluation.evaluator]: Inference done 5382/6982. Dataloading: 0.0047 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:04:06\n",
            "[05/24 09:33:01 d2.evaluation.evaluator]: Inference done 5414/6982. Dataloading: 0.0047 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:04:01\n",
            "[05/24 09:33:06 d2.evaluation.evaluator]: Inference done 5448/6982. Dataloading: 0.0047 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:03:56\n",
            "[05/24 09:33:11 d2.evaluation.evaluator]: Inference done 5480/6982. Dataloading: 0.0047 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:03:51\n",
            "[05/24 09:33:16 d2.evaluation.evaluator]: Inference done 5512/6982. Dataloading: 0.0047 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:03:46\n",
            "[05/24 09:33:21 d2.evaluation.evaluator]: Inference done 5546/6982. Dataloading: 0.0047 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:03:41\n",
            "[05/24 09:33:26 d2.evaluation.evaluator]: Inference done 5580/6982. Dataloading: 0.0047 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:03:36\n",
            "[05/24 09:33:31 d2.evaluation.evaluator]: Inference done 5612/6982. Dataloading: 0.0047 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:03:31\n",
            "[05/24 09:33:36 d2.evaluation.evaluator]: Inference done 5646/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:03:25\n",
            "[05/24 09:33:41 d2.evaluation.evaluator]: Inference done 5679/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:03:20\n",
            "[05/24 09:33:46 d2.evaluation.evaluator]: Inference done 5712/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:03:15\n",
            "[05/24 09:33:51 d2.evaluation.evaluator]: Inference done 5744/6982. Dataloading: 0.0047 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:03:10\n",
            "[05/24 09:33:56 d2.evaluation.evaluator]: Inference done 5778/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:03:05\n",
            "[05/24 09:34:01 d2.evaluation.evaluator]: Inference done 5811/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:03:00\n",
            "[05/24 09:34:07 d2.evaluation.evaluator]: Inference done 5843/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1541 s/iter. ETA=0:02:55\n",
            "[05/24 09:34:12 d2.evaluation.evaluator]: Inference done 5878/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:02:50\n",
            "[05/24 09:34:17 d2.evaluation.evaluator]: Inference done 5912/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:02:44\n",
            "[05/24 09:34:22 d2.evaluation.evaluator]: Inference done 5945/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:02:39\n",
            "[05/24 09:34:27 d2.evaluation.evaluator]: Inference done 5978/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:02:34\n",
            "[05/24 09:34:32 d2.evaluation.evaluator]: Inference done 6012/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:02:29\n",
            "[05/24 09:34:37 d2.evaluation.evaluator]: Inference done 6045/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:02:24\n",
            "[05/24 09:34:42 d2.evaluation.evaluator]: Inference done 6078/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:02:19\n",
            "[05/24 09:34:47 d2.evaluation.evaluator]: Inference done 6112/6982. Dataloading: 0.0046 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:02:13\n",
            "[05/24 09:34:53 d2.evaluation.evaluator]: Inference done 6147/6982. Dataloading: 0.0045 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:02:08\n",
            "[05/24 09:34:58 d2.evaluation.evaluator]: Inference done 6180/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:02:03\n",
            "[05/24 09:35:03 d2.evaluation.evaluator]: Inference done 6213/6982. Dataloading: 0.0046 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:01:58\n",
            "[05/24 09:35:08 d2.evaluation.evaluator]: Inference done 6246/6982. Dataloading: 0.0045 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:01:53\n",
            "[05/24 09:35:13 d2.evaluation.evaluator]: Inference done 6278/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:01:48\n",
            "[05/24 09:35:18 d2.evaluation.evaluator]: Inference done 6309/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:01:43\n",
            "[05/24 09:35:23 d2.evaluation.evaluator]: Inference done 6342/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:01:38\n",
            "[05/24 09:35:28 d2.evaluation.evaluator]: Inference done 6376/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:01:33\n",
            "[05/24 09:35:33 d2.evaluation.evaluator]: Inference done 6409/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:01:28\n",
            "[05/24 09:35:38 d2.evaluation.evaluator]: Inference done 6442/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1540 s/iter. ETA=0:01:23\n",
            "[05/24 09:35:43 d2.evaluation.evaluator]: Inference done 6477/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:01:17\n",
            "[05/24 09:35:48 d2.evaluation.evaluator]: Inference done 6509/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:01:12\n",
            "[05/24 09:35:53 d2.evaluation.evaluator]: Inference done 6542/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:01:07\n",
            "[05/24 09:35:58 d2.evaluation.evaluator]: Inference done 6576/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:01:02\n",
            "[05/24 09:36:03 d2.evaluation.evaluator]: Inference done 6610/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:00:57\n",
            "[05/24 09:36:09 d2.evaluation.evaluator]: Inference done 6642/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:00:52\n",
            "[05/24 09:36:14 d2.evaluation.evaluator]: Inference done 6675/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:00:47\n",
            "[05/24 09:36:19 d2.evaluation.evaluator]: Inference done 6710/6982. Dataloading: 0.0045 s/iter. Inference: 0.1488 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:00:41\n",
            "[05/24 09:36:24 d2.evaluation.evaluator]: Inference done 6742/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:00:36\n",
            "[05/24 09:36:29 d2.evaluation.evaluator]: Inference done 6775/6982. Dataloading: 0.0045 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:00:31\n",
            "[05/24 09:36:34 d2.evaluation.evaluator]: Inference done 6809/6982. Dataloading: 0.0044 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1538 s/iter. ETA=0:00:26\n",
            "[05/24 09:36:39 d2.evaluation.evaluator]: Inference done 6842/6982. Dataloading: 0.0044 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1538 s/iter. ETA=0:00:21\n",
            "[05/24 09:36:44 d2.evaluation.evaluator]: Inference done 6873/6982. Dataloading: 0.0044 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:00:16\n",
            "[05/24 09:36:49 d2.evaluation.evaluator]: Inference done 6907/6982. Dataloading: 0.0044 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1539 s/iter. ETA=0:00:11\n",
            "[05/24 09:36:54 d2.evaluation.evaluator]: Inference done 6942/6982. Dataloading: 0.0044 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1538 s/iter. ETA=0:00:06\n",
            "[05/24 09:36:59 d2.evaluation.evaluator]: Inference done 6974/6982. Dataloading: 0.0044 s/iter. Inference: 0.1489 s/iter. Eval: 0.0004 s/iter. Total: 0.1538 s/iter. ETA=0:00:01\n",
            "[05/24 09:37:00 d2.evaluation.evaluator]: Total inference time: 0:17:53.363774 (0.153843 s / iter per device, on 1 devices)\n",
            "[05/24 09:37:00 d2.evaluation.evaluator]: Total inference pure compute time: 0:17:18 (0.148867 s / iter per device, on 1 devices)\n",
            "[05/24 09:37:01 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/24 09:37:01 d2.evaluation.coco_evaluation]: Saving results to gdrive/MyDrive/data/coco-17/person/output_17_2/coco_instances_results.json\n",
            "[05/24 09:37:01 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.08s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/24 09:37:01 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[05/24 09:37:11 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 10.31 seconds.\n",
            "[05/24 09:37:12 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/24 09:37:13 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.93 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.009\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.007\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.004\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.008\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.002\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.006\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.007\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.004\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.007\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.009\n",
            "[05/24 09:37:13 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.590 | 0.934  | 0.679  | 0.353 | 0.619 | 0.782 |\n",
            "[05/24 09:37:13 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category      | AP    | category      | AP     | category       | AP    |\n",
            "|:--------------|:------|:--------------|:-------|:---------------|:------|\n",
            "| airplane      | 0.000 | apple         | 0.000  | backpack       | 0.000 |\n",
            "| banana        | 0.000 | baseball bat  | 0.000  | baseball glove | 0.000 |\n",
            "| bear          | 0.000 | bed           | 0.000  | bench          | 0.000 |\n",
            "| bicycle       | 0.000 | bird          | 0.000  | boat           | 0.000 |\n",
            "| book          | 0.000 | bottle        | 0.000  | bowl           | 0.000 |\n",
            "| broccoli      | 0.000 | bus           | 0.000  | cake           | 0.000 |\n",
            "| car           | 0.000 | carrot        | 0.000  | cat            | 0.000 |\n",
            "| cell phone    | 0.000 | chair         | 0.000  | clock          | 0.000 |\n",
            "| couch         | 0.000 | cow           | 0.000  | cup            | 0.000 |\n",
            "| dining table  | 0.000 | dog           | 0.000  | donut          | 0.000 |\n",
            "| elephant      | 0.000 | fire hydrant  | 0.000  | fork           | 0.000 |\n",
            "| frisbee       | 0.000 | giraffe       | 0.000  | hair drier     | 0.000 |\n",
            "| handbag       | 0.000 | horse         | 0.000  | hot dog        | 0.000 |\n",
            "| keyboard      | 0.000 | kite          | 0.000  | knife          | 0.000 |\n",
            "| laptop        | 0.000 | microwave     | 0.000  | motorcycle     | 0.000 |\n",
            "| mouse         | 0.000 | orange        | 0.000  | oven           | 0.000 |\n",
            "| parking meter | 0.000 | person        | 47.207 | pizza          | 0.000 |\n",
            "| potted plant  | 0.000 | refrigerator  | 0.000  | remote         | 0.000 |\n",
            "| sandwich      | 0.000 | scissors      | 0.000  | sheep          | 0.000 |\n",
            "| sink          | 0.000 | skateboard    | 0.000  | skis           | 0.000 |\n",
            "| snowboard     | 0.000 | spoon         | 0.000  | sports ball    | 0.000 |\n",
            "| stop sign     | 0.000 | suitcase      | 0.000  | surfboard      | 0.000 |\n",
            "| teddy bear    | 0.000 | tennis racket | 0.000  | tie            | 0.000 |\n",
            "| toaster       | 0.000 | toilet        | 0.000  | toothbrush     | 0.000 |\n",
            "| traffic light | 0.000 | train         | 0.000  | truck          | 0.000 |\n",
            "| tv            | 0.000 | umbrella      | 0.000  | vase           | 0.000 |\n",
            "| wine glass    | 0.000 | zebra         | 0.000  |                |       |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiments with dense posing**"
      ],
      "metadata": {
        "id": "Eg2asC4tDmhr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fmXLi5g3gsmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d44ce68-033a-4ca2-89c3-859069df6ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/24 09:37:15 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from gdrive/MyDrive/data/coco-17/person/output_17_3/model_final.pth ...\n",
            "WARNING [05/24 09:37:20 d2.data.datasets.coco]: gdrive/MyDrive/data/coco-17/person/coco.json contains 66502 annotations, but only 66349 of them match to images in the file.\n",
            "[05/24 09:37:20 d2.data.datasets.coco]: Loaded 6982 images in COCO format from gdrive/MyDrive/data/coco-17/person/coco.json\n",
            "[05/24 09:37:20 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/24 09:37:20 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/24 09:37:20 d2.data.common]: Serializing 6982 elements to byte tensors and concatenating them all ...\n",
            "[05/24 09:37:21 d2.data.common]: Serialized dataset takes 5.15 MiB\n",
            "[05/24 09:37:21 d2.evaluation.evaluator]: Start inference on 6982 batches\n",
            "[05/24 09:37:22 d2.evaluation.evaluator]: Inference done 11/6982. Dataloading: 0.0018 s/iter. Inference: 0.1422 s/iter. Eval: 0.0003 s/iter. Total: 0.1443 s/iter. ETA=0:16:45\n",
            "[05/24 09:37:28 d2.evaluation.evaluator]: Inference done 46/6982. Dataloading: 0.0020 s/iter. Inference: 0.1423 s/iter. Eval: 0.0003 s/iter. Total: 0.1447 s/iter. ETA=0:16:43\n",
            "[05/24 09:37:33 d2.evaluation.evaluator]: Inference done 81/6982. Dataloading: 0.0029 s/iter. Inference: 0.1420 s/iter. Eval: 0.0003 s/iter. Total: 0.1453 s/iter. ETA=0:16:42\n",
            "[05/24 09:37:38 d2.evaluation.evaluator]: Inference done 115/6982. Dataloading: 0.0032 s/iter. Inference: 0.1424 s/iter. Eval: 0.0003 s/iter. Total: 0.1461 s/iter. ETA=0:16:43\n",
            "[05/24 09:37:43 d2.evaluation.evaluator]: Inference done 150/6982. Dataloading: 0.0032 s/iter. Inference: 0.1420 s/iter. Eval: 0.0003 s/iter. Total: 0.1457 s/iter. ETA=0:16:35\n",
            "[05/24 09:37:48 d2.evaluation.evaluator]: Inference done 183/6982. Dataloading: 0.0036 s/iter. Inference: 0.1427 s/iter. Eval: 0.0003 s/iter. Total: 0.1468 s/iter. ETA=0:16:37\n",
            "[05/24 09:37:53 d2.evaluation.evaluator]: Inference done 216/6982. Dataloading: 0.0039 s/iter. Inference: 0.1432 s/iter. Eval: 0.0003 s/iter. Total: 0.1476 s/iter. ETA=0:16:38\n",
            "[05/24 09:37:58 d2.evaluation.evaluator]: Inference done 250/6982. Dataloading: 0.0037 s/iter. Inference: 0.1435 s/iter. Eval: 0.0003 s/iter. Total: 0.1477 s/iter. ETA=0:16:34\n",
            "[05/24 09:38:03 d2.evaluation.evaluator]: Inference done 284/6982. Dataloading: 0.0035 s/iter. Inference: 0.1438 s/iter. Eval: 0.0003 s/iter. Total: 0.1478 s/iter. ETA=0:16:29\n",
            "[05/24 09:38:08 d2.evaluation.evaluator]: Inference done 318/6982. Dataloading: 0.0035 s/iter. Inference: 0.1440 s/iter. Eval: 0.0003 s/iter. Total: 0.1480 s/iter. ETA=0:16:26\n",
            "[05/24 09:38:13 d2.evaluation.evaluator]: Inference done 351/6982. Dataloading: 0.0036 s/iter. Inference: 0.1444 s/iter. Eval: 0.0003 s/iter. Total: 0.1484 s/iter. ETA=0:16:24\n",
            "[05/24 09:38:18 d2.evaluation.evaluator]: Inference done 385/6982. Dataloading: 0.0035 s/iter. Inference: 0.1446 s/iter. Eval: 0.0003 s/iter. Total: 0.1485 s/iter. ETA=0:16:19\n",
            "[05/24 09:38:23 d2.evaluation.evaluator]: Inference done 419/6982. Dataloading: 0.0034 s/iter. Inference: 0.1446 s/iter. Eval: 0.0003 s/iter. Total: 0.1485 s/iter. ETA=0:16:14\n",
            "[05/24 09:38:28 d2.evaluation.evaluator]: Inference done 452/6982. Dataloading: 0.0036 s/iter. Inference: 0.1447 s/iter. Eval: 0.0003 s/iter. Total: 0.1488 s/iter. ETA=0:16:11\n",
            "[05/24 09:38:33 d2.evaluation.evaluator]: Inference done 486/6982. Dataloading: 0.0036 s/iter. Inference: 0.1448 s/iter. Eval: 0.0003 s/iter. Total: 0.1488 s/iter. ETA=0:16:06\n",
            "[05/24 09:38:38 d2.evaluation.evaluator]: Inference done 520/6982. Dataloading: 0.0035 s/iter. Inference: 0.1450 s/iter. Eval: 0.0003 s/iter. Total: 0.1489 s/iter. ETA=0:16:02\n",
            "[05/24 09:38:43 d2.evaluation.evaluator]: Inference done 553/6982. Dataloading: 0.0036 s/iter. Inference: 0.1450 s/iter. Eval: 0.0003 s/iter. Total: 0.1491 s/iter. ETA=0:15:58\n",
            "[05/24 09:38:48 d2.evaluation.evaluator]: Inference done 586/6982. Dataloading: 0.0037 s/iter. Inference: 0.1451 s/iter. Eval: 0.0003 s/iter. Total: 0.1493 s/iter. ETA=0:15:55\n",
            "[05/24 09:38:53 d2.evaluation.evaluator]: Inference done 621/6982. Dataloading: 0.0036 s/iter. Inference: 0.1449 s/iter. Eval: 0.0003 s/iter. Total: 0.1490 s/iter. ETA=0:15:47\n",
            "[05/24 09:38:58 d2.evaluation.evaluator]: Inference done 655/6982. Dataloading: 0.0036 s/iter. Inference: 0.1450 s/iter. Eval: 0.0003 s/iter. Total: 0.1490 s/iter. ETA=0:15:42\n",
            "[05/24 09:39:04 d2.evaluation.evaluator]: Inference done 688/6982. Dataloading: 0.0036 s/iter. Inference: 0.1451 s/iter. Eval: 0.0003 s/iter. Total: 0.1493 s/iter. ETA=0:15:39\n",
            "[05/24 09:39:09 d2.evaluation.evaluator]: Inference done 721/6982. Dataloading: 0.0036 s/iter. Inference: 0.1453 s/iter. Eval: 0.0003 s/iter. Total: 0.1494 s/iter. ETA=0:15:35\n",
            "[05/24 09:39:14 d2.evaluation.evaluator]: Inference done 756/6982. Dataloading: 0.0035 s/iter. Inference: 0.1452 s/iter. Eval: 0.0003 s/iter. Total: 0.1492 s/iter. ETA=0:15:28\n",
            "[05/24 09:39:19 d2.evaluation.evaluator]: Inference done 789/6982. Dataloading: 0.0036 s/iter. Inference: 0.1452 s/iter. Eval: 0.0003 s/iter. Total: 0.1493 s/iter. ETA=0:15:24\n",
            "[05/24 09:39:24 d2.evaluation.evaluator]: Inference done 822/6982. Dataloading: 0.0036 s/iter. Inference: 0.1454 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:15:20\n",
            "[05/24 09:39:29 d2.evaluation.evaluator]: Inference done 856/6982. Dataloading: 0.0036 s/iter. Inference: 0.1454 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:15:15\n",
            "[05/24 09:39:34 d2.evaluation.evaluator]: Inference done 890/6982. Dataloading: 0.0036 s/iter. Inference: 0.1455 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:15:10\n",
            "[05/24 09:39:39 d2.evaluation.evaluator]: Inference done 923/6982. Dataloading: 0.0036 s/iter. Inference: 0.1455 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:15:06\n",
            "[05/24 09:39:44 d2.evaluation.evaluator]: Inference done 957/6982. Dataloading: 0.0036 s/iter. Inference: 0.1455 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:15:01\n",
            "[05/24 09:39:49 d2.evaluation.evaluator]: Inference done 992/6982. Dataloading: 0.0035 s/iter. Inference: 0.1455 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:14:55\n",
            "[05/24 09:39:54 d2.evaluation.evaluator]: Inference done 1025/6982. Dataloading: 0.0036 s/iter. Inference: 0.1456 s/iter. Eval: 0.0003 s/iter. Total: 0.1497 s/iter. ETA=0:14:51\n",
            "[05/24 09:39:59 d2.evaluation.evaluator]: Inference done 1059/6982. Dataloading: 0.0036 s/iter. Inference: 0.1457 s/iter. Eval: 0.0003 s/iter. Total: 0.1497 s/iter. ETA=0:14:46\n",
            "[05/24 09:40:04 d2.evaluation.evaluator]: Inference done 1094/6982. Dataloading: 0.0035 s/iter. Inference: 0.1455 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:14:40\n",
            "[05/24 09:40:09 d2.evaluation.evaluator]: Inference done 1128/6982. Dataloading: 0.0035 s/iter. Inference: 0.1455 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:14:35\n",
            "[05/24 09:40:15 d2.evaluation.evaluator]: Inference done 1161/6982. Dataloading: 0.0036 s/iter. Inference: 0.1455 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:14:30\n",
            "[05/24 09:40:20 d2.evaluation.evaluator]: Inference done 1195/6982. Dataloading: 0.0035 s/iter. Inference: 0.1455 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:14:25\n",
            "[05/24 09:40:25 d2.evaluation.evaluator]: Inference done 1229/6982. Dataloading: 0.0035 s/iter. Inference: 0.1455 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:14:19\n",
            "[05/24 09:40:30 d2.evaluation.evaluator]: Inference done 1262/6982. Dataloading: 0.0035 s/iter. Inference: 0.1456 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:14:15\n",
            "[05/24 09:40:35 d2.evaluation.evaluator]: Inference done 1297/6982. Dataloading: 0.0035 s/iter. Inference: 0.1455 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:14:10\n",
            "[05/24 09:40:40 d2.evaluation.evaluator]: Inference done 1331/6982. Dataloading: 0.0035 s/iter. Inference: 0.1456 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:14:04\n",
            "[05/24 09:40:45 d2.evaluation.evaluator]: Inference done 1366/6982. Dataloading: 0.0035 s/iter. Inference: 0.1455 s/iter. Eval: 0.0003 s/iter. Total: 0.1494 s/iter. ETA=0:13:59\n",
            "[05/24 09:40:50 d2.evaluation.evaluator]: Inference done 1399/6982. Dataloading: 0.0036 s/iter. Inference: 0.1455 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:13:54\n",
            "[05/24 09:40:55 d2.evaluation.evaluator]: Inference done 1435/6982. Dataloading: 0.0035 s/iter. Inference: 0.1454 s/iter. Eval: 0.0003 s/iter. Total: 0.1493 s/iter. ETA=0:13:48\n",
            "[05/24 09:41:00 d2.evaluation.evaluator]: Inference done 1469/6982. Dataloading: 0.0035 s/iter. Inference: 0.1454 s/iter. Eval: 0.0003 s/iter. Total: 0.1494 s/iter. ETA=0:13:43\n",
            "[05/24 09:41:05 d2.evaluation.evaluator]: Inference done 1502/6982. Dataloading: 0.0035 s/iter. Inference: 0.1455 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:13:39\n",
            "[05/24 09:41:10 d2.evaluation.evaluator]: Inference done 1535/6982. Dataloading: 0.0035 s/iter. Inference: 0.1456 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:13:34\n",
            "[05/24 09:41:16 d2.evaluation.evaluator]: Inference done 1570/6982. Dataloading: 0.0035 s/iter. Inference: 0.1456 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:13:29\n",
            "[05/24 09:41:21 d2.evaluation.evaluator]: Inference done 1603/6982. Dataloading: 0.0035 s/iter. Inference: 0.1457 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:13:24\n",
            "[05/24 09:41:26 d2.evaluation.evaluator]: Inference done 1636/6982. Dataloading: 0.0035 s/iter. Inference: 0.1457 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:13:19\n",
            "[05/24 09:41:31 d2.evaluation.evaluator]: Inference done 1670/6982. Dataloading: 0.0034 s/iter. Inference: 0.1457 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:13:14\n",
            "[05/24 09:41:36 d2.evaluation.evaluator]: Inference done 1704/6982. Dataloading: 0.0034 s/iter. Inference: 0.1457 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:13:09\n",
            "[05/24 09:41:41 d2.evaluation.evaluator]: Inference done 1738/6982. Dataloading: 0.0034 s/iter. Inference: 0.1457 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:13:04\n",
            "[05/24 09:41:46 d2.evaluation.evaluator]: Inference done 1772/6982. Dataloading: 0.0034 s/iter. Inference: 0.1457 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:12:59\n",
            "[05/24 09:41:51 d2.evaluation.evaluator]: Inference done 1806/6982. Dataloading: 0.0034 s/iter. Inference: 0.1457 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:12:53\n",
            "[05/24 09:41:56 d2.evaluation.evaluator]: Inference done 1840/6982. Dataloading: 0.0034 s/iter. Inference: 0.1456 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:12:48\n",
            "[05/24 09:42:01 d2.evaluation.evaluator]: Inference done 1874/6982. Dataloading: 0.0034 s/iter. Inference: 0.1456 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:12:43\n",
            "[05/24 09:42:06 d2.evaluation.evaluator]: Inference done 1908/6982. Dataloading: 0.0034 s/iter. Inference: 0.1456 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:12:38\n",
            "[05/24 09:42:11 d2.evaluation.evaluator]: Inference done 1942/6982. Dataloading: 0.0034 s/iter. Inference: 0.1456 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:12:33\n",
            "[05/24 09:42:16 d2.evaluation.evaluator]: Inference done 1975/6982. Dataloading: 0.0034 s/iter. Inference: 0.1457 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:12:28\n",
            "[05/24 09:42:21 d2.evaluation.evaluator]: Inference done 2009/6982. Dataloading: 0.0034 s/iter. Inference: 0.1457 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:12:23\n",
            "[05/24 09:42:26 d2.evaluation.evaluator]: Inference done 2044/6982. Dataloading: 0.0034 s/iter. Inference: 0.1457 s/iter. Eval: 0.0003 s/iter. Total: 0.1495 s/iter. ETA=0:12:18\n",
            "[05/24 09:42:32 d2.evaluation.evaluator]: Inference done 2077/6982. Dataloading: 0.0034 s/iter. Inference: 0.1457 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:12:13\n",
            "[05/24 09:42:37 d2.evaluation.evaluator]: Inference done 2110/6982. Dataloading: 0.0034 s/iter. Inference: 0.1458 s/iter. Eval: 0.0003 s/iter. Total: 0.1496 s/iter. ETA=0:12:08\n",
            "[05/24 09:42:42 d2.evaluation.evaluator]: Inference done 2142/6982. Dataloading: 0.0035 s/iter. Inference: 0.1458 s/iter. Eval: 0.0003 s/iter. Total: 0.1497 s/iter. ETA=0:12:04\n",
            "[05/24 09:42:47 d2.evaluation.evaluator]: Inference done 2176/6982. Dataloading: 0.0034 s/iter. Inference: 0.1458 s/iter. Eval: 0.0003 s/iter. Total: 0.1497 s/iter. ETA=0:11:59\n",
            "[05/24 09:42:52 d2.evaluation.evaluator]: Inference done 2210/6982. Dataloading: 0.0034 s/iter. Inference: 0.1458 s/iter. Eval: 0.0003 s/iter. Total: 0.1497 s/iter. ETA=0:11:54\n",
            "[05/24 09:42:57 d2.evaluation.evaluator]: Inference done 2243/6982. Dataloading: 0.0035 s/iter. Inference: 0.1458 s/iter. Eval: 0.0003 s/iter. Total: 0.1498 s/iter. ETA=0:11:49\n",
            "[05/24 09:43:02 d2.evaluation.evaluator]: Inference done 2276/6982. Dataloading: 0.0035 s/iter. Inference: 0.1459 s/iter. Eval: 0.0003 s/iter. Total: 0.1498 s/iter. ETA=0:11:45\n",
            "[05/24 09:43:07 d2.evaluation.evaluator]: Inference done 2310/6982. Dataloading: 0.0035 s/iter. Inference: 0.1459 s/iter. Eval: 0.0003 s/iter. Total: 0.1498 s/iter. ETA=0:11:40\n",
            "[05/24 09:43:12 d2.evaluation.evaluator]: Inference done 2343/6982. Dataloading: 0.0035 s/iter. Inference: 0.1459 s/iter. Eval: 0.0003 s/iter. Total: 0.1499 s/iter. ETA=0:11:35\n",
            "[05/24 09:43:17 d2.evaluation.evaluator]: Inference done 2376/6982. Dataloading: 0.0035 s/iter. Inference: 0.1459 s/iter. Eval: 0.0003 s/iter. Total: 0.1499 s/iter. ETA=0:11:30\n",
            "[05/24 09:43:22 d2.evaluation.evaluator]: Inference done 2410/6982. Dataloading: 0.0035 s/iter. Inference: 0.1459 s/iter. Eval: 0.0003 s/iter. Total: 0.1499 s/iter. ETA=0:11:25\n",
            "[05/24 09:43:27 d2.evaluation.evaluator]: Inference done 2444/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1499 s/iter. ETA=0:11:20\n",
            "[05/24 09:43:32 d2.evaluation.evaluator]: Inference done 2477/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1499 s/iter. ETA=0:11:15\n",
            "[05/24 09:43:37 d2.evaluation.evaluator]: Inference done 2510/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:11:10\n",
            "[05/24 09:43:42 d2.evaluation.evaluator]: Inference done 2544/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:11:05\n",
            "[05/24 09:43:48 d2.evaluation.evaluator]: Inference done 2578/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:11:00\n",
            "[05/24 09:43:53 d2.evaluation.evaluator]: Inference done 2611/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:10:55\n",
            "[05/24 09:43:58 d2.evaluation.evaluator]: Inference done 2645/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:10:50\n",
            "[05/24 09:44:03 d2.evaluation.evaluator]: Inference done 2679/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:10:45\n",
            "[05/24 09:44:08 d2.evaluation.evaluator]: Inference done 2713/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:10:40\n",
            "[05/24 09:44:13 d2.evaluation.evaluator]: Inference done 2747/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:10:35\n",
            "[05/24 09:44:18 d2.evaluation.evaluator]: Inference done 2782/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1499 s/iter. ETA=0:10:29\n",
            "[05/24 09:44:23 d2.evaluation.evaluator]: Inference done 2815/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1499 s/iter. ETA=0:10:24\n",
            "[05/24 09:44:28 d2.evaluation.evaluator]: Inference done 2848/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:10:20\n",
            "[05/24 09:44:33 d2.evaluation.evaluator]: Inference done 2881/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:10:15\n",
            "[05/24 09:44:38 d2.evaluation.evaluator]: Inference done 2915/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:10:09\n",
            "[05/24 09:44:43 d2.evaluation.evaluator]: Inference done 2948/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:10:05\n",
            "[05/24 09:44:48 d2.evaluation.evaluator]: Inference done 2981/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1501 s/iter. ETA=0:10:00\n",
            "[05/24 09:44:53 d2.evaluation.evaluator]: Inference done 3015/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:09:55\n",
            "[05/24 09:44:58 d2.evaluation.evaluator]: Inference done 3049/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:09:50\n",
            "[05/24 09:45:03 d2.evaluation.evaluator]: Inference done 3083/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:09:44\n",
            "[05/24 09:45:09 d2.evaluation.evaluator]: Inference done 3118/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:09:39\n",
            "[05/24 09:45:14 d2.evaluation.evaluator]: Inference done 3152/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:09:34\n",
            "[05/24 09:45:19 d2.evaluation.evaluator]: Inference done 3185/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:09:29\n",
            "[05/24 09:45:24 d2.evaluation.evaluator]: Inference done 3220/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:09:24\n",
            "[05/24 09:45:29 d2.evaluation.evaluator]: Inference done 3255/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1499 s/iter. ETA=0:09:18\n",
            "[05/24 09:45:34 d2.evaluation.evaluator]: Inference done 3288/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:09:13\n",
            "[05/24 09:45:39 d2.evaluation.evaluator]: Inference done 3322/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:09:08\n",
            "[05/24 09:45:44 d2.evaluation.evaluator]: Inference done 3356/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:09:03\n",
            "[05/24 09:45:49 d2.evaluation.evaluator]: Inference done 3390/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:08:58\n",
            "[05/24 09:45:54 d2.evaluation.evaluator]: Inference done 3423/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:08:53\n",
            "[05/24 09:45:59 d2.evaluation.evaluator]: Inference done 3457/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:08:48\n",
            "[05/24 09:46:04 d2.evaluation.evaluator]: Inference done 3491/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:08:43\n",
            "[05/24 09:46:09 d2.evaluation.evaluator]: Inference done 3523/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:08:38\n",
            "[05/24 09:46:14 d2.evaluation.evaluator]: Inference done 3556/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:08:34\n",
            "[05/24 09:46:19 d2.evaluation.evaluator]: Inference done 3590/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:08:28\n",
            "[05/24 09:46:25 d2.evaluation.evaluator]: Inference done 3624/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:08:23\n",
            "[05/24 09:46:30 d2.evaluation.evaluator]: Inference done 3658/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:08:18\n",
            "[05/24 09:46:35 d2.evaluation.evaluator]: Inference done 3692/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:08:13\n",
            "[05/24 09:46:40 d2.evaluation.evaluator]: Inference done 3726/6982. Dataloading: 0.0035 s/iter. Inference: 0.1460 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:08:08\n",
            "[05/24 09:46:45 d2.evaluation.evaluator]: Inference done 3758/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1501 s/iter. ETA=0:08:03\n",
            "[05/24 09:46:50 d2.evaluation.evaluator]: Inference done 3792/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1501 s/iter. ETA=0:07:58\n",
            "[05/24 09:46:55 d2.evaluation.evaluator]: Inference done 3827/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:07:53\n",
            "[05/24 09:47:00 d2.evaluation.evaluator]: Inference done 3860/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1501 s/iter. ETA=0:07:48\n",
            "[05/24 09:47:05 d2.evaluation.evaluator]: Inference done 3894/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1501 s/iter. ETA=0:07:43\n",
            "[05/24 09:47:10 d2.evaluation.evaluator]: Inference done 3928/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:07:38\n",
            "[05/24 09:47:15 d2.evaluation.evaluator]: Inference done 3962/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1500 s/iter. ETA=0:07:33\n",
            "[05/24 09:47:21 d2.evaluation.evaluator]: Inference done 3995/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1501 s/iter. ETA=0:07:28\n",
            "[05/24 09:47:26 d2.evaluation.evaluator]: Inference done 4029/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1501 s/iter. ETA=0:07:23\n",
            "[05/24 09:47:31 d2.evaluation.evaluator]: Inference done 4062/6982. Dataloading: 0.0035 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1501 s/iter. ETA=0:07:18\n",
            "[05/24 09:47:36 d2.evaluation.evaluator]: Inference done 4095/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:07:13\n",
            "[05/24 09:47:41 d2.evaluation.evaluator]: Inference done 4129/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1501 s/iter. ETA=0:07:08\n",
            "[05/24 09:47:46 d2.evaluation.evaluator]: Inference done 4163/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1501 s/iter. ETA=0:07:03\n",
            "[05/24 09:47:51 d2.evaluation.evaluator]: Inference done 4197/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:06:58\n",
            "[05/24 09:47:56 d2.evaluation.evaluator]: Inference done 4230/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:06:53\n",
            "[05/24 09:48:01 d2.evaluation.evaluator]: Inference done 4264/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:06:48\n",
            "[05/24 09:48:06 d2.evaluation.evaluator]: Inference done 4298/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:06:43\n",
            "[05/24 09:48:11 d2.evaluation.evaluator]: Inference done 4331/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:06:38\n",
            "[05/24 09:48:16 d2.evaluation.evaluator]: Inference done 4365/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:06:33\n",
            "[05/24 09:48:21 d2.evaluation.evaluator]: Inference done 4399/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:06:27\n",
            "[05/24 09:48:27 d2.evaluation.evaluator]: Inference done 4432/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:06:23\n",
            "[05/24 09:48:32 d2.evaluation.evaluator]: Inference done 4466/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:06:17\n",
            "[05/24 09:48:37 d2.evaluation.evaluator]: Inference done 4500/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:06:12\n",
            "[05/24 09:48:42 d2.evaluation.evaluator]: Inference done 4534/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:06:07\n",
            "[05/24 09:48:47 d2.evaluation.evaluator]: Inference done 4568/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:06:02\n",
            "[05/24 09:48:52 d2.evaluation.evaluator]: Inference done 4602/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:05:57\n",
            "[05/24 09:48:57 d2.evaluation.evaluator]: Inference done 4636/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:05:52\n",
            "[05/24 09:49:02 d2.evaluation.evaluator]: Inference done 4669/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:05:47\n",
            "[05/24 09:49:07 d2.evaluation.evaluator]: Inference done 4703/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:05:42\n",
            "[05/24 09:49:12 d2.evaluation.evaluator]: Inference done 4737/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:05:37\n",
            "[05/24 09:49:17 d2.evaluation.evaluator]: Inference done 4771/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:05:32\n",
            "[05/24 09:49:22 d2.evaluation.evaluator]: Inference done 4804/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:05:27\n",
            "[05/24 09:49:27 d2.evaluation.evaluator]: Inference done 4838/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:05:22\n",
            "[05/24 09:49:33 d2.evaluation.evaluator]: Inference done 4872/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:05:16\n",
            "[05/24 09:49:38 d2.evaluation.evaluator]: Inference done 4905/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:05:12\n",
            "[05/24 09:49:43 d2.evaluation.evaluator]: Inference done 4938/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:05:07\n",
            "[05/24 09:49:48 d2.evaluation.evaluator]: Inference done 4972/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:05:01\n",
            "[05/24 09:49:53 d2.evaluation.evaluator]: Inference done 5005/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:04:57\n",
            "[05/24 09:49:58 d2.evaluation.evaluator]: Inference done 5038/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:04:52\n",
            "[05/24 09:50:03 d2.evaluation.evaluator]: Inference done 5072/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:04:46\n",
            "[05/24 09:50:08 d2.evaluation.evaluator]: Inference done 5106/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:04:41\n",
            "[05/24 09:50:13 d2.evaluation.evaluator]: Inference done 5139/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:04:36\n",
            "[05/24 09:50:18 d2.evaluation.evaluator]: Inference done 5173/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:04:31\n",
            "[05/24 09:50:23 d2.evaluation.evaluator]: Inference done 5207/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:04:26\n",
            "[05/24 09:50:28 d2.evaluation.evaluator]: Inference done 5241/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:04:21\n",
            "[05/24 09:50:33 d2.evaluation.evaluator]: Inference done 5275/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:04:16\n",
            "[05/24 09:50:39 d2.evaluation.evaluator]: Inference done 5309/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:04:11\n",
            "[05/24 09:50:44 d2.evaluation.evaluator]: Inference done 5343/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:04:06\n",
            "[05/24 09:50:49 d2.evaluation.evaluator]: Inference done 5376/6982. Dataloading: 0.0037 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:04:01\n",
            "[05/24 09:50:54 d2.evaluation.evaluator]: Inference done 5411/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:03:55\n",
            "[05/24 09:50:59 d2.evaluation.evaluator]: Inference done 5445/6982. Dataloading: 0.0036 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1502 s/iter. ETA=0:03:50\n",
            "[05/24 09:51:04 d2.evaluation.evaluator]: Inference done 5478/6982. Dataloading: 0.0037 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:03:45\n",
            "[05/24 09:51:09 d2.evaluation.evaluator]: Inference done 5511/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:03:41\n",
            "[05/24 09:51:14 d2.evaluation.evaluator]: Inference done 5545/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:03:35\n",
            "[05/24 09:51:19 d2.evaluation.evaluator]: Inference done 5579/6982. Dataloading: 0.0037 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:03:30\n",
            "[05/24 09:51:24 d2.evaluation.evaluator]: Inference done 5612/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:03:25\n",
            "[05/24 09:51:29 d2.evaluation.evaluator]: Inference done 5646/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:03:20\n",
            "[05/24 09:51:34 d2.evaluation.evaluator]: Inference done 5680/6982. Dataloading: 0.0036 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:03:15\n",
            "[05/24 09:51:39 d2.evaluation.evaluator]: Inference done 5713/6982. Dataloading: 0.0037 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:03:10\n",
            "[05/24 09:51:44 d2.evaluation.evaluator]: Inference done 5747/6982. Dataloading: 0.0037 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:03:05\n",
            "[05/24 09:51:49 d2.evaluation.evaluator]: Inference done 5781/6982. Dataloading: 0.0037 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:03:00\n",
            "[05/24 09:51:55 d2.evaluation.evaluator]: Inference done 5814/6982. Dataloading: 0.0037 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:02:55\n",
            "[05/24 09:52:00 d2.evaluation.evaluator]: Inference done 5848/6982. Dataloading: 0.0037 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:02:50\n",
            "[05/24 09:52:05 d2.evaluation.evaluator]: Inference done 5882/6982. Dataloading: 0.0037 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:02:45\n",
            "[05/24 09:52:10 d2.evaluation.evaluator]: Inference done 5915/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:02:40\n",
            "[05/24 09:52:15 d2.evaluation.evaluator]: Inference done 5948/6982. Dataloading: 0.0037 s/iter. Inference: 0.1461 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:02:35\n",
            "[05/24 09:52:20 d2.evaluation.evaluator]: Inference done 5980/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:02:30\n",
            "[05/24 09:52:25 d2.evaluation.evaluator]: Inference done 6013/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:02:25\n",
            "[05/24 09:52:30 d2.evaluation.evaluator]: Inference done 6046/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:02:20\n",
            "[05/24 09:52:35 d2.evaluation.evaluator]: Inference done 6080/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:02:15\n",
            "[05/24 09:52:40 d2.evaluation.evaluator]: Inference done 6113/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:02:10\n",
            "[05/24 09:52:45 d2.evaluation.evaluator]: Inference done 6148/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:02:05\n",
            "[05/24 09:52:50 d2.evaluation.evaluator]: Inference done 6183/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:02:00\n",
            "[05/24 09:52:55 d2.evaluation.evaluator]: Inference done 6216/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1503 s/iter. ETA=0:01:55\n",
            "[05/24 09:53:00 d2.evaluation.evaluator]: Inference done 6249/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:01:50\n",
            "[05/24 09:53:06 d2.evaluation.evaluator]: Inference done 6283/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:01:45\n",
            "[05/24 09:53:11 d2.evaluation.evaluator]: Inference done 6316/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:01:40\n",
            "[05/24 09:53:16 d2.evaluation.evaluator]: Inference done 6348/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:01:35\n",
            "[05/24 09:53:21 d2.evaluation.evaluator]: Inference done 6382/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:01:30\n",
            "[05/24 09:53:26 d2.evaluation.evaluator]: Inference done 6416/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:01:25\n",
            "[05/24 09:53:31 d2.evaluation.evaluator]: Inference done 6448/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:01:20\n",
            "[05/24 09:53:36 d2.evaluation.evaluator]: Inference done 6483/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:01:15\n",
            "[05/24 09:53:41 d2.evaluation.evaluator]: Inference done 6517/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:01:09\n",
            "[05/24 09:53:46 d2.evaluation.evaluator]: Inference done 6551/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:01:04\n",
            "[05/24 09:53:51 d2.evaluation.evaluator]: Inference done 6585/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:00:59\n",
            "[05/24 09:53:56 d2.evaluation.evaluator]: Inference done 6620/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:00:54\n",
            "[05/24 09:54:02 d2.evaluation.evaluator]: Inference done 6654/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:00:49\n",
            "[05/24 09:54:07 d2.evaluation.evaluator]: Inference done 6687/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:00:44\n",
            "[05/24 09:54:12 d2.evaluation.evaluator]: Inference done 6721/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:00:39\n",
            "[05/24 09:54:17 d2.evaluation.evaluator]: Inference done 6755/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:00:34\n",
            "[05/24 09:54:22 d2.evaluation.evaluator]: Inference done 6789/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:00:29\n",
            "[05/24 09:54:27 d2.evaluation.evaluator]: Inference done 6822/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:00:24\n",
            "[05/24 09:54:32 d2.evaluation.evaluator]: Inference done 6856/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:00:18\n",
            "[05/24 09:54:37 d2.evaluation.evaluator]: Inference done 6889/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:00:13\n",
            "[05/24 09:54:42 d2.evaluation.evaluator]: Inference done 6922/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:00:09\n",
            "[05/24 09:54:47 d2.evaluation.evaluator]: Inference done 6956/6982. Dataloading: 0.0037 s/iter. Inference: 0.1462 s/iter. Eval: 0.0003 s/iter. Total: 0.1504 s/iter. ETA=0:00:03\n",
            "[05/24 09:54:51 d2.evaluation.evaluator]: Total inference time: 0:17:29.602151 (0.150437 s / iter per device, on 1 devices)\n",
            "[05/24 09:54:51 d2.evaluation.evaluator]: Total inference pure compute time: 0:17:00 (0.146234 s / iter per device, on 1 devices)\n",
            "[05/24 09:54:51 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/24 09:54:52 d2.evaluation.coco_evaluation]: Saving results to gdrive/MyDrive/data/coco-17/person/output_17_3/coco_instances_results.json\n",
            "[05/24 09:54:52 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.07s)\n",
            "creating index...\n",
            "index created!\n",
            "[05/24 09:54:52 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[05/24 09:55:04 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 12.27 seconds.\n",
            "[05/24 09:55:04 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[05/24 09:55:05 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.87 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.003\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.007\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.002\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.006\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.006\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.004\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.007\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.008\n",
            "[05/24 09:55:05 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.552 | 0.951  | 0.590  | 0.306 | 0.589 | 0.734 |\n",
            "[05/24 09:55:05 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category      | AP    | category      | AP     | category       | AP    |\n",
            "|:--------------|:------|:--------------|:-------|:---------------|:------|\n",
            "| airplane      | 0.000 | apple         | 0.000  | backpack       | 0.000 |\n",
            "| banana        | 0.000 | baseball bat  | 0.000  | baseball glove | 0.000 |\n",
            "| bear          | 0.000 | bed           | 0.000  | bench          | 0.000 |\n",
            "| bicycle       | 0.000 | bird          | 0.000  | boat           | 0.000 |\n",
            "| book          | 0.000 | bottle        | 0.000  | bowl           | 0.000 |\n",
            "| broccoli      | 0.000 | bus           | 0.000  | cake           | 0.000 |\n",
            "| car           | 0.000 | carrot        | 0.000  | cat            | 0.000 |\n",
            "| cell phone    | 0.000 | chair         | 0.000  | clock          | 0.000 |\n",
            "| couch         | 0.000 | cow           | 0.000  | cup            | 0.000 |\n",
            "| dining table  | 0.000 | dog           | 0.000  | donut          | 0.000 |\n",
            "| elephant      | 0.000 | fire hydrant  | 0.000  | fork           | 0.000 |\n",
            "| frisbee       | 0.000 | giraffe       | 0.000  | hair drier     | 0.000 |\n",
            "| handbag       | 0.000 | horse         | 0.000  | hot dog        | 0.000 |\n",
            "| keyboard      | 0.000 | kite          | 0.000  | knife          | 0.000 |\n",
            "| laptop        | 0.000 | microwave     | 0.000  | motorcycle     | 0.000 |\n",
            "| mouse         | 0.000 | orange        | 0.000  | oven           | 0.000 |\n",
            "| parking meter | 0.000 | person        | 44.199 | pizza          | 0.000 |\n",
            "| potted plant  | 0.000 | refrigerator  | 0.000  | remote         | 0.000 |\n",
            "| sandwich      | 0.000 | scissors      | 0.000  | sheep          | 0.000 |\n",
            "| sink          | 0.000 | skateboard    | 0.000  | skis           | 0.000 |\n",
            "| snowboard     | 0.000 | spoon         | 0.000  | sports ball    | 0.000 |\n",
            "| stop sign     | 0.000 | suitcase      | 0.000  | surfboard      | 0.000 |\n",
            "| teddy bear    | 0.000 | tennis racket | 0.000  | tie            | 0.000 |\n",
            "| toaster       | 0.000 | toilet        | 0.000  | toothbrush     | 0.000 |\n",
            "| traffic light | 0.000 | train         | 0.000  | truck          | 0.000 |\n",
            "| tv            | 0.000 | umbrella      | 0.000  | vase           | 0.000 |\n",
            "| wine glass    | 0.000 | zebra         | 0.000  |                |       |\n"
          ]
        }
      ],
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "#Prediction With Dense Layering\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "evaluator = COCOEvaluator(dataset_name, output_dir=cfg.OUTPUT_DIR)\n",
        "val_loader = build_detection_test_loader(cfg, dataset_name)\n",
        "results = inference_on_dataset(predictor.model, val_loader, evaluator)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}